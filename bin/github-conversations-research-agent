#!/usr/bin/env ruby

# bin/github-conversations-research-agent: Multi-turn research agent for GitHub conversations
#
# This script implements a research workflow that:
# 1. Takes a natural language question and performs initial semantic search
# 2. Generates clarifying questions and opens them in $EDITOR
# 3. Performs iterative deep research based on clarifications
# 4. Produces a final well-formatted Markdown report citing all sources
#
# Usage: github-conversations-research-agent "QUESTION" --collection COLLECTION [options]
#
# Options:
#   --collection NAME        Qdrant collection name (required)
#   -n, --limit N           Max results per search (default: 10)
#   --max-depth N           Max deep-research passes (default: 10)
#   --editor-file PATH      Use fixed file instead of Tempfile
#   --verbose               Show debug logs
#   --fast-model MODEL      Fast LLM model for light reasoning
#   --reasoning-model MODEL Reasoning LLM model for complex analysis
#
# The script uses the existing bin/semantic-search-github-conversations and
# bin/fetch-github-conversation scripts to gather context, and integrates
# with the llm CLI for AI-powered analysis.

require "json"
require "logger"
require "open3"
require "optparse"
require "tempfile"
require "shellwords"

# Load vendored Pocketflow library
require_relative "../lib/pocketflow"

# Set up global logger
LOG = Logger.new($stdout)
LOG.level = Logger::INFO  # Default level, will be changed to DEBUG with --verbose

# === Embedded Prompt Templates ===

ASK_CLARIFY_PROMPT = <<~PROMPT
You are an expert analyst reviewing a research question and initial findings from GitHub conversations.

## Research Question
{{question}}

## Initial Findings Summary
{{initial_findings}}

Based on the question and initial findings, generate up to 4 clarifying questions that would help you better understand the issue and provide more targeted research. When creating your questions, focus on uncovering:

1. Objectives: What is the ultimate goal or outcome the requester wants?
2. Scope: What boundaries or constraints should the research stay within?
3. Data & Format: What data sources, formats, or tools are expected or preferred?
4. Success Criteria: What does a “good” answer look like to the requester?

Format your response as a numbered list with clear, specific questions. Each question should be on its own line starting with a number. The instructions should ask for inline answers to these questions.
PROMPT

DEEP_RESEARCH_PROMPT = <<~PROMPT
You are an expert researcher mining GitHub conversations for actionable evidence.

## Original Question
{{question}}

## User Clarifications
{{clarifications}}

## Findings So Far
{{findings_summary}}

## Prior Search Queries
{{previous_queries}}

**Goal**
Identify the most significant information gaps and craft one new natural-language search query (≤ 2 sentences, no operators) that will surface GitHub conversations to close them.
Gaps may include — but are not limited to — missing implementation details, unclear project status (Implemented · In Progress · Proposed · Abandoned · Unknown), decisions, trade-offs, or alternative solutions.

When project “doneness” is uncertain, bias your query toward finding evidence that proves or disproves implementation (e.g. merged PRs, release notes, deployment comments). Otherwise, target whatever gap is highest-impact.

Return **only** the search query text.

*Examples*
- “Confirmation that client-side rate limiting is merged and live”
- “Alternative approaches to large-table migration performance”
- “Discussion showing why the authentication redesign was abandoned”
PROMPT

FINAL_REPORT_PROMPT = <<~PROMPT
You are an expert analyst preparing a comprehensive Markdown report.

## Original Question
{{question}}

## User Clarifications
{{clarifications}}

## Research Corpus
{{all_findings}}

Produce a well-structured Markdown report with these sections:

### 1. Executive Summary
Brief answer (≤ 5 sentences).

### 2. Key Findings
Bullet list of the main insights. For each item include:
- **Topic** (one line)
- **Implementation Status** in parentheses — *Implemented · In Progress · Proposed · Abandoned · Unknown*
- One-sentence takeaway
- GitHub URL

### 3. Detailed Analysis
Expanded discussion grouped by logical themes (performance, security, etc.).
For any feature or project, explicitly state its **Implementation Status** and quote a short evidence snippet (commit SHA, “Merged” event, comment, etc.) supporting that status.

### 4. Sources
Plain list of every GitHub conversation URL cited.

**Style guide**

* Use proper Markdown headings (`##`, `###`) and omit horizontal rules (`---`).
* Every factual claim must be backed by an inline citation (full URL).
* If status cannot be confirmed, mark it **Unknown** and note what evidence is missing.

Return only the Markdown document—no extra commentary.
PROMPT

# === Helper Methods ===

# Public: Runs a shell command and returns stdout. Aborts if the command fails.
#
# cmd - The shell command to run (String).
#
# Returns the standard output of the command (String).
# Raises SystemExit if the command fails.
def run_cmd(cmd)
  stdout, stderr, status = Open3.capture3(cmd)
  abort "Command failed: #{cmd}\n#{stderr}" unless status.success?
  stdout.strip
end

# Public: Checks if a required command-line dependency is available in PATH.
#
# cmd - The String name of the command to check.
#
# Returns nothing. Exits if not found.
def check_dependency(cmd)
  system("which #{cmd} > /dev/null 2>&1") || abort("Required dependency '#{cmd}' not found in PATH.")
end

# Public: Gets the editor using Git's resolution order.
#
# Returns the editor command as a String.
def get_git_editor
  # Follow Git's editor resolution order:
  # 1. GIT_EDITOR environment variable
  # 2. core.editor config value
  # 3. VISUAL environment variable
  # 4. EDITOR environment variable
  # 5. Fall back to system default

  return ENV["GIT_EDITOR"] if ENV["GIT_EDITOR"] && !ENV["GIT_EDITOR"].strip.empty?

  # Try git config core.editor
  git_config_editor = `git config --get core.editor 2>/dev/null`.strip
  return git_config_editor unless git_config_editor.empty?

  return ENV["VISUAL"] if ENV["VISUAL"] && !ENV["VISUAL"].strip.empty?
  return ENV["EDITOR"] if ENV["EDITOR"] && !ENV["EDITOR"].strip.empty?

  # Fall back to nano as a sensible default
  "nano"
end

# Public: Opens a text editor with the given content and returns the edited result.
#
# text - The String text to edit.
# file_path - Optional String path to use instead of a temporary file.
#
# Returns the edited text as a String.
def edit_text(text, file_path = nil)
  if file_path
    File.write(file_path, text)
    tmp_path = file_path
  else
    tmp = Tempfile.create(["research_edit", ".md"])
    tmp.puts text
    tmp.flush
    tmp_path = tmp.path
  end

  # Get editor using Git's resolution order
  editor = get_git_editor()

  # Open editor
  unless system("#{editor} #{tmp_path}")
    abort "Editor command failed: #{editor}"
  end

  # Read the edited content
  File.read(tmp_path)
ensure
  tmp&.close unless file_path
end

# Public: Fills in template variables in a prompt string.
#
# template - The String template with {{variable}} placeholders.
# variables - Hash of variable names to values.
#
# Returns the filled template as a String.
def fill_template(template, variables)
  result = template.dup
  variables.each do |key, value|
    result.gsub!("{{#{key}}}", value.to_s)
  end
  result
end

# Public: Calls the LLM CLI for chat completion.
#
# prompt - The String prompt to send.
# model - Optional String model name.
#
# Returns the LLM response as a String.
def call_llm(prompt, model = nil)
  check_dependency("llm") # Check only when needed
  model_flag = model ? "-m #{Shellwords.escape(model)}" : ""
  cmd = "llm #{model_flag} <<< #{Shellwords.escape(prompt)}"
  run_cmd(cmd)
end

# Public: Extracts conversation metadata from GitHub conversation data.
#
# conversation_data - The parsed JSON from fetch-github-conversation.
#
# Returns a Hash with conversation metadata and logging information.
def extract_conversation_metadata(conversation_data)
  # Determine type and extract conversation metadata
  conversation_type = if conversation_data["issue"]
    "issue"
  elsif conversation_data["pr"]
    "pull request"
  elsif conversation_data["discussion"]
    "discussion"
  else
    "unknown"
  end

  # Get the actual conversation object based on type
  conversation_obj = conversation_data["issue"] || conversation_data["pr"] || conversation_data["discussion"] || {}

  # Extract metadata for logging and return
  {
    type: conversation_type,
    title: conversation_obj["title"] || "Unknown title",
    state: conversation_obj["state"] || "unknown",
    comments_count: conversation_data["comments"]&.length || 0
  }
end

# === Pocketflow Nodes ===

class InitialResearchNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context for use in exec
    LOG.info "=== INITIAL RESEARCH PHASE ==="
    LOG.info "Starting initial semantic search for: #{shared[:question]}"
    LOG.debug "Collection: #{shared[:collection]}"
    LOG.debug "Max results: #{shared[:top_k]}"

    question = shared[:question]
    collection = shared[:collection]
    top_k = shared[:top_k]
    script_dir = shared[:script_dir]

    # Run semantic search
    search_cmd = "#{script_dir}/semantic-search-github-conversations #{Shellwords.escape(question)} --collection #{Shellwords.escape(collection)} --limit #{top_k} --format json"
    LOG.debug "Running search command: #{search_cmd}"

    search_output = run_cmd(search_cmd)
    search_results = JSON.parse(search_output)

    LOG.info "Found #{search_results.length} initial results"
    # LOG.debug example: showing detailed search results when verbose logging is enabled
    LOG.debug do
      result_details = search_results.map.with_index do |result, i|
        "  #{i + 1}. URL: #{result.dig('payload', 'url')}\n" \
        "     Score: #{result['score']}\n" \
        "     Summary: #{result.dig('payload', 'summary')&.slice(0, 100)}..."
      end.join("\n\n")
      "Initial search results:\n#{result_details}"
    end

    search_results
  end

  def exec(search_results)
    LOG.info "Fetching detailed conversation data for #{search_results.length} results..."

    # Fetch detailed conversation data for each result
    enriched_results = []

    search_results.each_with_index do |result, i|
      url = result.dig("payload", "url")
      next unless url

      LOG.debug "Fetching details for result #{i + 1}/#{search_results.length}: #{url}"

      begin
        fetch_cmd = "#{@shared[:script_dir]}/fetch-github-conversation #{Shellwords.escape(url)}"
        conversation_json = run_cmd(fetch_cmd)
        conversation_data = JSON.parse(conversation_json)

        metadata = extract_conversation_metadata(conversation_data)

        LOG.debug do
          "✓ Successfully fetched: #{metadata[:title]}\n" \
          "  Type: #{metadata[:type]}\n" \
          "  State: #{metadata[:state]}\n" \
          "  Comments: #{metadata[:comments_count]}"
        end

        enriched_results << {
          url: url,
          summary: result.dig("payload", "summary") || "",
          score: result["score"],
          conversation: conversation_data
        }
      rescue => e
        LOG.warn "Failed to fetch #{url}: #{e.message}"
      end
    end

    LOG.info "Successfully enriched #{enriched_results.length}/#{search_results.length} conversations"
    enriched_results
  end

  def post(shared, prep_res, exec_res)
    shared[:memory] ||= {}
    shared[:memory][:hits] = exec_res
    shared[:memory][:notes] = []
    shared[:memory][:search_queries] = [shared[:question]]

    LOG.info "✓ Initial research complete: #{exec_res.length} conversations collected"
    LOG.debug "Moving to clarifying questions phase..."

    "default"
  end
end

class AskClarifyingNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    LOG.info "=== CLARIFYING QUESTIONS PHASE ==="
    LOG.info "Generating clarifying questions based on initial findings..."

    # Summarize initial findings
    initial_findings = shared[:memory][:hits].map do |hit|
      "- #{hit[:url]}: #{hit[:summary]}"
    end.join("\n")

    LOG.debug do
      "Initial findings summary:\n#{initial_findings}"
    end

    # Fill template and call LLM
    prompt = fill_template(ASK_CLARIFY_PROMPT, {
      question: shared[:question],
      initial_findings: initial_findings
    })

    LOG.debug "Calling LLM to generate clarifying questions..."
    # Use fast model for clarifying questions - this is light reasoning to generate questions based on initial findings
    llm_response = call_llm(prompt, shared[:models][:fast])

    LOG.info "Generated clarifying questions for user review"
    LOG.debug do
      "Generated questions:\n#{'=' * 60}\n#{llm_response}\n#{'=' * 60}"
    end

    llm_response
  end

  def exec(clarifying_questions)
    LOG.info "Opening editor for user to answer clarifying questions..."

    # Prepare editor content
    editor_content = <<~CONTENT
Please review the following questions and provide inline answers to help focus the research:

#{clarifying_questions}
CONTENT

    # Open editor
    edited_content = edit_text(editor_content, @shared[:editor_file])

    LOG.info "User provided clarifications"
    LOG.debug do
      "User clarifications:\n#{'=' * 60}\n#{edited_content}\n#{'=' * 60}"
    end

    edited_content
  end

  def post(shared, prep_res, exec_res)
    shared[:clarifications] = exec_res
    LOG.info "✓ Clarifications collected, proceeding to deep research"
    LOG.debug "Moving to deep research phase..."

    "default"
  end
end

class DeepResearchNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    depth = shared[:current_depth] || 0
    max_depth = shared[:max_depth]

    LOG.info "=== DEEP RESEARCH PHASE (Iteration #{depth + 1}/#{max_depth}) ==="
    LOG.info "Starting deep research iteration #{depth + 1}/#{max_depth}"

    # Check if we've reached max depth
    if depth >= max_depth
      LOG.info "Maximum depth reached, moving to final report"
      return nil
    end

    # Prepare context for LLM
    findings_summary = shared[:memory][:notes].join("\n\n")
    previous_queries = shared[:memory][:search_queries].join(", ")

    LOG.debug do
      "Current research context:\n" \
      "  Previous queries: #{previous_queries}\n" \
      "  Total conversations found so far: #{shared[:memory][:hits].length}\n" \
      "  Research notes accumulated: #{shared[:memory][:notes].length}"
    end

    prompt = fill_template(DEEP_RESEARCH_PROMPT, {
      question: shared[:question],
      clarifications: shared[:clarifications] || "",
      findings_summary: findings_summary,
      previous_queries: previous_queries
    })

    LOG.debug "Calling LLM to generate refined search query..."
    # Use fast model for search query generation - this is light reasoning to refine search based on context
    refined_query = call_llm(prompt, shared[:models][:fast])

    LOG.info "Generated refined search query: \"#{refined_query}\""

    refined_query
  end

  def exec(refined_query)
    return [] if refined_query.nil?

    collection = @shared[:collection]
    top_k = @shared[:top_k]
    script_dir = @shared[:script_dir]

    LOG.info "Running semantic search with refined query..."

    # Run semantic search with refined query
    search_cmd = "#{script_dir}/semantic-search-github-conversations #{Shellwords.escape(refined_query)} --collection #{Shellwords.escape(collection)} --limit #{top_k} --format json"
    LOG.debug "Search command: #{search_cmd}"

    search_output = run_cmd(search_cmd)
    search_results = JSON.parse(search_output)

    LOG.debug "Raw search results: #{search_results.length} conversations found"

    # Deduplicate URLs we already have
    existing_urls = @shared[:memory][:hits].map { |hit| hit[:url] }.to_set
    new_results = search_results.reject { |result| existing_urls.include?(result.dig("payload", "url")) }

    LOG.info "Found #{new_results.length} new conversations after deduplication"

    if new_results.empty?
      LOG.info "No new results found - all were duplicates"
      return []
    end

    LOG.debug do
      result_details = new_results.map.with_index do |result, i|
        "  #{i + 1}. URL: #{result.dig('payload', 'url')}\n" \
        "     Score: #{result['score']}\n" \
        "     Summary: #{result.dig('payload', 'summary')&.slice(0, 100)}..."
      end.join("\n\n")
      "New results found:\n#{result_details}"
    end

    # Fetch detailed data for new results
    new_enriched = []
    new_results.each_with_index do |result, i|
      url = result.dig("payload", "url")
      next unless url

      LOG.debug "Fetching details for new result #{i + 1}/#{new_results.length}: #{url}"

      begin
        fetch_cmd = "#{@shared[:script_dir]}/fetch-github-conversation #{Shellwords.escape(url)}"
        conversation_json = run_cmd(fetch_cmd)
        conversation_data = JSON.parse(conversation_json)

        metadata = extract_conversation_metadata(conversation_data)

        LOG.debug do
          "✓ Successfully fetched: #{metadata[:title]}\n" \
          "  Type: #{metadata[:type]}\n" \
          "  State: #{metadata[:state]}\n" \
          "  Comments: #{metadata[:comments_count]}"
        end

        new_enriched << {
          url: url,
          summary: result.dig("payload", "summary") || "",
          score: result["score"],
          conversation: conversation_data
        }
      rescue => e
        LOG.warn "Failed to fetch #{url}: #{e.message}"
      end
    end

    LOG.info "Successfully enriched #{new_enriched.length}/#{new_results.length} new conversations"
    new_enriched
  end

  def post(shared, prep_res, exec_res)
    return "final" if prep_res.nil? # Max depth reached

    # Add new findings to memory
    shared[:memory][:hits].concat(exec_res)
    shared[:memory][:search_queries] << prep_res

    # Add research notes
    if exec_res.any?
      notes = exec_res.map { |hit| "#{hit[:url]}: #{hit[:summary]}" }.join("\n")
      shared[:memory][:notes] << "Research iteration: #{notes}"
      LOG.info "Added #{exec_res.length} new conversations to memory"
    else
      LOG.info "No new conversations added this iteration"
    end

    # Increment depth
    shared[:current_depth] = (shared[:current_depth] || 0) + 1

    LOG.debug do
      "Current memory state:\n" \
      "  Total conversations: #{shared[:memory][:hits].length}\n" \
      "  Total search queries: #{shared[:memory][:search_queries].length}\n" \
      "  Current depth: #{shared[:current_depth]}/#{shared[:max_depth]}"
    end

    # Continue if under max depth, otherwise go to final report
    if shared[:current_depth] < shared[:max_depth] && exec_res.any?
      LOG.info "Continuing to next deep research iteration..."
      "default" # Continue deep research loop
    else
      LOG.info "Deep research complete, moving to final report..."
      "final"
    end
  end
end

class FinalReportNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    LOG.info "=== FINAL REPORT PHASE ==="
    LOG.info "Generating final report from all gathered data..."

    LOG.info "Research summary: #{shared[:memory][:hits].length} conversations analyzed, #{shared[:memory][:search_queries].length} queries used, #{shared[:current_depth] || 0} deep research iterations"

    LOG.debug do
      sources_list = shared[:memory][:hits].map.with_index do |hit, i|
        "  #{i + 1}. #{hit[:url]} (score: #{hit[:score]})"
      end.join("\n")
      "All conversation sources:\n#{sources_list}"
    end

    # Compile all findings
    all_findings = shared[:memory][:hits].map do |hit|
      <<~FINDING
      **Source**: #{hit[:url]}
      **Summary**: #{hit[:summary]}
      **Relevance Score**: #{hit[:score]}

      **Conversation Details**:
      #{JSON.pretty_generate(hit[:conversation])}
      FINDING
    end.join("\n\n---\n\n")

    prompt = fill_template(FINAL_REPORT_PROMPT, {
      question: shared[:question],
      clarifications: shared[:clarifications] || "None provided",
      all_findings: all_findings
    })

    LOG.debug "Calling LLM to generate final report..."
    prompt
  end

  def exec(prompt)
    # Use reasoning model for final report - this requires complex analysis and synthesis of all gathered data
    call_llm(prompt, @shared[:models][:reasoning])
  end

  def post(shared, prep_res, exec_res)
    LOG.info "=== FINAL REPORT ==="
    puts exec_res
    LOG.info "✓ Research complete! Total conversations analyzed: #{shared[:memory][:hits].length}"

    "default"
  end
end

# === Main Script ===

# Parse command-line options
options = {
  collection: nil,
  limit: 10,
  max_depth: 10,
  editor_file: nil,
  verbose: false,
  fast_model: ENV["FAST_LLM_MODEL"],
  reasoning_model: ENV["LLM_MODEL"]
}

opt_parser = OptionParser.new do |opts|
  opts.banner = "Usage: #{File.basename($0)} \"QUESTION\" --collection COLLECTION [options]"

  opts.on("--collection NAME", "Qdrant collection name (required)") do |v|
    options[:collection] = v
  end

  opts.on("-n", "--limit N", Integer, "Max results per search (default: 10)") do |v|
    options[:limit] = v
  end

  opts.on("--max-depth N", Integer, "Max deep-research passes (default: 10)") do |v|
    options[:max_depth] = v
  end

  opts.on("--editor-file PATH", "Use fixed file instead of Tempfile") do |v|
    options[:editor_file] = v
  end

  opts.on("--verbose", "Show debug logs") do
    options[:verbose] = true
  end

  opts.on("--fast-model MODEL", "Fast LLM model for light reasoning (default: ENV['FAST_LLM_MODEL'] or llm default)") do |v|
    options[:fast_model] = v
  end

  opts.on("--reasoning-model MODEL", "Reasoning LLM model for complex analysis (default: ENV['LLM_MODEL'] or llm default)") do |v|
    options[:reasoning_model] = v
  end

  opts.on("-h", "--help", "Show this help message") do
    puts opts
    exit
  end
end

begin
  opt_parser.parse!
rescue OptionParser::InvalidOption => e
  abort "#{e.message}\n\n#{opt_parser}"
end

# Set logger level based on verbose flag
LOG.level = options[:verbose] ? Logger::DEBUG : Logger::INFO

# Validate required arguments
if ARGV.empty?
  abort opt_parser.to_s
end

question = ARGV.join(" ")

if question.strip.empty?
  abort "Error: Empty question provided"
end

unless options[:collection]
  abort "Error: --collection is required\n\n#{opt_parser}"
end

# Set up shared context
script_dir = File.expand_path(File.dirname(__FILE__))

shared = {
  question: question,
  collection: options[:collection],
  top_k: options[:limit],
  max_depth: options[:max_depth],
  editor_file: options[:editor_file],
  verbose: options[:verbose],
  models: {
    fast: options[:fast_model],
    reasoning: options[:reasoning_model]
  },
  script_dir: script_dir
}

# Build the workflow
initial_node = InitialResearchNode.new
clarify_node = AskClarifyingNode.new
deep_node = DeepResearchNode.new
final_node = FinalReportNode.new

# Link the nodes
initial_node.next(clarify_node)
clarify_node.next(deep_node)
deep_node.next(deep_node) # Loop back to itself
deep_node.on("final", final_node)

# Create and run the flow
flow = Pocketflow::Flow.new(initial_node)

begin
  LOG.info "=== GITHUB CONVERSATIONS RESEARCH AGENT ==="
  LOG.info "Question: #{question}"
  LOG.info "Collection: #{options[:collection]}"
  LOG.info "Max results per search: #{options[:limit]}"
  LOG.info "Max deep research iterations: #{options[:max_depth]}"
  LOG.info "Fast model: #{shared[:models][:fast] || 'default'}"
  LOG.info "Reasoning model: #{shared[:models][:reasoning] || 'default'}"

  flow.run(shared)
rescue Interrupt
  LOG.error "\nResearch interrupted by user"
  exit 1
rescue => e
  LOG.error "Error: #{e.message}"
  LOG.debug e.backtrace.join("\n") if shared[:verbose]
  exit 1
end
