#!/usr/bin/env ruby

# bin/github-conversations-research-agent: Multi-turn research agent for GitHub conversations
#
# This script implements a research workflow that:
# 1. Takes a natural language question and performs initial semantic search
# 2. Generates clarifying questions and opens them in $EDITOR
# 3. Performs iterative deep research based on clarifications
# 4. Produces a final well-formatted Markdown report citing all sources
#
# Usage: github-conversations-research-agent "QUESTION" --collection COLLECTION [options]
#
# Options:
#   --collection NAME        Qdrant collection name (required)
#   -n, --limit N           Max results per search (default: 10)
#   --max-depth N           Max deep-research passes (default: 10)
#   --editor-file PATH      Use fixed file instead of Tempfile
#   --verbose               Show debug logs
#   --llm-model MODEL       LLM model to use
#
# The script uses the existing bin/semantic-search-github-conversations and
# bin/fetch-github-conversation scripts to gather context, and integrates
# with the llm CLI for AI-powered analysis.

require "json"
require "logger"
require "open3"
require "optparse"
require "tempfile"
require "shellwords"

# Load vendored Pocketflow library
require_relative "../lib/pocketflow"

# Set up global logger
LOG = Logger.new($stdout)
LOG.level = Logger::INFO  # Default level, will be changed to DEBUG with --verbose

# === Embedded Prompt Templates ===

ASK_CLARIFY_PROMPT = <<~PROMPT
You are an expert analyst reviewing a research question and initial findings from GitHub conversations.

## Research Question
{{question}}

## Initial Findings Summary
{{initial_findings}}

Based on the question and initial findings, generate up to 4 clarifying questions that would help you better understand the issue and provide more targeted research. Focus on:

1. Missing context or background information
2. Specific technical details that need clarification
3. Stakeholder perspectives or decision criteria
4. Timeline or process-related questions

Format your response as a numbered list with clear, specific questions. Each question should be on its own line starting with a number. The instructions should ask for inline answers to these questions.
PROMPT

DEEP_RESEARCH_PROMPT = <<~PROMPT
You are an expert researcher analyzing GitHub conversations. Based on the original question, clarifications provided, and findings so far, determine what additional information to search for.

## Original Question
{{question}}

## User's Clarifications
{{clarifications}}

## Research Findings So Far
{{findings_summary}}

## Previous Search Queries
{{previous_queries}}

Based on this context, generate a simple English search query that would help find additional relevant GitHub conversations. The query should be:

- A natural language sentence or phrase (no special syntax, filters, or technical operators)
- 1-2 sentences maximum
- Written as if asking a question or describing what you're looking for
- Focused on filling gaps in current understanding, finding related issues, discovering implementation details, or uncovering related problems

Examples of good queries:
- "How to implement rate limiting in API endpoints"
- "Database migration performance issues with large tables"
- "User authentication failing after recent updates"

Focus on:
- Filling gaps in current understanding
- Finding related issues or discussions
- Discovering implementation details or decisions
- Uncovering related problems or solutions

Return only the search query text as a simple English sentence, nothing else.
PROMPT

FINAL_REPORT_PROMPT = <<~PROMPT
You are an expert analyst preparing a comprehensive research report. Using all the gathered information, create a well-structured Markdown report that answers the original research question.

## Original Question
{{question}}

## User's Clarifications
{{clarifications}}

## All Research Findings
{{all_findings}}

Create a comprehensive Markdown report that:

1. **Executive Summary**: Brief answer to the original question
2. **Key Findings**: Main insights discovered during research
3. **Detailed Analysis**: In-depth analysis with supporting evidence
4. **Sources**: List all GitHub conversation URLs used as sources

Structure the report with clear headings and cite specific GitHub conversations using their full URLs. Make sure every major claim is supported by evidence from the GitHub conversations.

Format as clean Markdown with proper headings, bullet points, and citations.
PROMPT

# === Helper Methods ===

# Public: Runs a shell command and returns stdout. Aborts if the command fails.
#
# cmd - The shell command to run (String).
#
# Returns the standard output of the command (String).
# Raises SystemExit if the command fails.
def run_cmd(cmd)
  stdout, stderr, status = Open3.capture3(cmd)
  abort "Command failed: #{cmd}\n#{stderr}" unless status.success?
  stdout.strip
end

# Public: Checks if a required command-line dependency is available in PATH.
#
# cmd - The String name of the command to check.
#
# Returns nothing. Exits if not found.
def check_dependency(cmd)
  system("which #{cmd} > /dev/null 2>&1") || abort("Required dependency '#{cmd}' not found in PATH.")
end

# Public: Gets the editor using Git's resolution order.
#
# Returns the editor command as a String.
def get_git_editor
  # Follow Git's editor resolution order:
  # 1. GIT_EDITOR environment variable
  # 2. core.editor config value
  # 3. VISUAL environment variable
  # 4. EDITOR environment variable
  # 5. Fall back to system default

  return ENV["GIT_EDITOR"] if ENV["GIT_EDITOR"] && !ENV["GIT_EDITOR"].strip.empty?

  # Try git config core.editor
  git_config_editor = `git config --get core.editor 2>/dev/null`.strip
  return git_config_editor unless git_config_editor.empty?

  return ENV["VISUAL"] if ENV["VISUAL"] && !ENV["VISUAL"].strip.empty?
  return ENV["EDITOR"] if ENV["EDITOR"] && !ENV["EDITOR"].strip.empty?

  # Fall back to nano as a sensible default
  "nano"
end

# Public: Opens a text editor with the given content and returns the edited result.
#
# text - The String text to edit.
# file_path - Optional String path to use instead of a temporary file.
#
# Returns the edited text as a String.
def edit_text(text, file_path = nil)
  if file_path
    File.write(file_path, text)
    tmp_path = file_path
  else
    tmp = Tempfile.create(["research_edit", ".md"])
    tmp.puts text
    tmp.flush
    tmp_path = tmp.path
  end

  # Get editor using Git's resolution order
  editor = get_git_editor()

  # Open editor
  unless system("#{editor} #{tmp_path}")
    abort "Editor command failed: #{editor}"
  end

  # Read the edited content
  File.read(tmp_path)
ensure
  tmp&.close unless file_path
end

# Public: Fills in template variables in a prompt string.
#
# template - The String template with {{variable}} placeholders.
# variables - Hash of variable names to values.
#
# Returns the filled template as a String.
def fill_template(template, variables)
  result = template.dup
  variables.each do |key, value|
    result.gsub!("{{#{key}}}", value.to_s)
  end
  result
end

# Public: Calls the LLM CLI for chat completion.
#
# prompt - The String prompt to send.
# model - Optional String model name.
#
# Returns the LLM response as a String.
def call_llm(prompt, model = nil)
  check_dependency("llm") # Check only when needed
  model_flag = model ? "-m #{Shellwords.escape(model)}" : ""
  cmd = "llm #{model_flag} <<< #{Shellwords.escape(prompt)}"
  run_cmd(cmd)
end

# Public: Extracts conversation metadata from GitHub conversation data.
#
# conversation_data - The parsed JSON from fetch-github-conversation.
#
# Returns a Hash with conversation metadata and logging information.
def extract_conversation_metadata(conversation_data)
  # Determine type and extract conversation metadata
  conversation_type = if conversation_data["issue"]
    "issue"
  elsif conversation_data["pr"]
    "pull request"
  elsif conversation_data["discussion"]
    "discussion"
  else
    "unknown"
  end

  # Get the actual conversation object based on type
  conversation_obj = conversation_data["issue"] || conversation_data["pr"] || conversation_data["discussion"] || {}

  # Extract metadata for logging and return
  {
    type: conversation_type,
    title: conversation_obj["title"] || "Unknown title",
    state: conversation_obj["state"] || "unknown",
    comments_count: conversation_data["comments"]&.length || 0
  }
end

# === Pocketflow Nodes ===

class InitialResearchNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context for use in exec
    LOG.info "=== INITIAL RESEARCH PHASE ==="
    LOG.info "Starting initial semantic search for: #{shared[:question]}"
    LOG.debug "Collection: #{shared[:collection]}"
    LOG.debug "Max results: #{shared[:top_k]}"

    question = shared[:question]
    collection = shared[:collection]
    top_k = shared[:top_k]
    script_dir = shared[:script_dir]

    # Run semantic search
    search_cmd = "#{script_dir}/semantic-search-github-conversations #{Shellwords.escape(question)} --collection #{Shellwords.escape(collection)} --limit #{top_k} --format json"
    LOG.debug "Running search command: #{search_cmd}"

    search_output = run_cmd(search_cmd)
    search_results = JSON.parse(search_output)

    LOG.info "Found #{search_results.length} initial results"
    # LOG.debug example: showing detailed search results when verbose logging is enabled
    LOG.debug do
      result_details = search_results.map.with_index do |result, i|
        "  #{i + 1}. URL: #{result.dig('payload', 'url')}\n" \
        "     Score: #{result['score']}\n" \
        "     Summary: #{result.dig('payload', 'summary')&.slice(0, 100)}..."
      end.join("\n\n")
      "Initial search results:\n#{result_details}"
    end

    search_results
  end

  def exec(search_results)
    LOG.info "Fetching detailed conversation data for #{search_results.length} results..."

    # Fetch detailed conversation data for each result
    enriched_results = []

    search_results.each_with_index do |result, i|
      url = result.dig("payload", "url")
      next unless url

      LOG.debug "Fetching details for result #{i + 1}/#{search_results.length}: #{url}"

      begin
        fetch_cmd = "#{@shared[:script_dir]}/fetch-github-conversation #{Shellwords.escape(url)}"
        conversation_json = run_cmd(fetch_cmd)
        conversation_data = JSON.parse(conversation_json)

        metadata = extract_conversation_metadata(conversation_data)

        LOG.debug do
          "✓ Successfully fetched: #{metadata[:title]}\n" \
          "  Type: #{metadata[:type]}\n" \
          "  State: #{metadata[:state]}\n" \
          "  Comments: #{metadata[:comments_count]}"
        end

        enriched_results << {
          url: url,
          summary: result.dig("payload", "summary") || "",
          score: result["score"],
          conversation: conversation_data
        }
      rescue => e
        LOG.warn "Failed to fetch #{url}: #{e.message}"
      end
    end

    LOG.info "Successfully enriched #{enriched_results.length}/#{search_results.length} conversations"
    enriched_results
  end

  def post(shared, prep_res, exec_res)
    shared[:memory] ||= {}
    shared[:memory][:hits] = exec_res
    shared[:memory][:notes] = []
    shared[:memory][:search_queries] = [shared[:question]]

    LOG.info "✓ Initial research complete: #{exec_res.length} conversations collected"
    LOG.debug "Moving to clarifying questions phase..."

    "default"
  end
end

class AskClarifyingNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    LOG.info "=== CLARIFYING QUESTIONS PHASE ==="
    LOG.info "Generating clarifying questions based on initial findings..."

    # Summarize initial findings
    initial_findings = shared[:memory][:hits].map do |hit|
      "- #{hit[:url]}: #{hit[:summary]}"
    end.join("\n")

    LOG.debug do
      "Initial findings summary:\n#{initial_findings}"
    end

    # Fill template and call LLM
    prompt = fill_template(ASK_CLARIFY_PROMPT, {
      question: shared[:question],
      initial_findings: initial_findings
    })

    LOG.debug "Calling LLM to generate clarifying questions..."
    llm_response = call_llm(prompt, shared[:llm_model])

    LOG.info "Generated clarifying questions for user review"
    LOG.debug do
      "Generated questions:\n#{'=' * 60}\n#{llm_response}\n#{'=' * 60}"
    end

    llm_response
  end

  def exec(clarifying_questions)
    LOG.info "Opening editor for user to answer clarifying questions..."

    # Prepare editor content
    editor_content = <<~CONTENT
Please review the following questions and provide inline answers to help focus the research:

#{clarifying_questions}
CONTENT

    # Open editor
    edited_content = edit_text(editor_content, @shared[:editor_file])

    LOG.info "User provided clarifications"
    LOG.debug do
      "User clarifications:\n#{'=' * 60}\n#{edited_content}\n#{'=' * 60}"
    end

    edited_content
  end

  def post(shared, prep_res, exec_res)
    shared[:clarifications] = exec_res
    LOG.info "✓ Clarifications collected, proceeding to deep research"
    LOG.debug "Moving to deep research phase..."

    "default"
  end
end

class DeepResearchNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    depth = shared[:current_depth] || 0
    max_depth = shared[:max_depth]

    LOG.info "=== DEEP RESEARCH PHASE (Iteration #{depth + 1}/#{max_depth}) ==="
    LOG.info "Starting deep research iteration #{depth + 1}/#{max_depth}"

    # Check if we've reached max depth
    if depth >= max_depth
      LOG.info "Maximum depth reached, moving to final report"
      return nil
    end

    # Prepare context for LLM
    findings_summary = shared[:memory][:notes].join("\n\n")
    previous_queries = shared[:memory][:search_queries].join(", ")

    LOG.debug do
      "Current research context:\n" \
      "  Previous queries: #{previous_queries}\n" \
      "  Total conversations found so far: #{shared[:memory][:hits].length}\n" \
      "  Research notes accumulated: #{shared[:memory][:notes].length}"
    end

    prompt = fill_template(DEEP_RESEARCH_PROMPT, {
      question: shared[:question],
      clarifications: shared[:clarifications] || "",
      findings_summary: findings_summary,
      previous_queries: previous_queries
    })

    LOG.debug "Calling LLM to generate refined search query..."
    # Generate refined search query
    refined_query = call_llm(prompt, shared[:llm_model])

    LOG.info "Generated refined search query: \"#{refined_query}\""

    refined_query
  end

  def exec(refined_query)
    return [] if refined_query.nil?

    collection = @shared[:collection]
    top_k = @shared[:top_k]
    script_dir = @shared[:script_dir]

    LOG.info "Running semantic search with refined query..."

    # Run semantic search with refined query
    search_cmd = "#{script_dir}/semantic-search-github-conversations #{Shellwords.escape(refined_query)} --collection #{Shellwords.escape(collection)} --limit #{top_k} --format json"
    LOG.debug "Search command: #{search_cmd}"

    search_output = run_cmd(search_cmd)
    search_results = JSON.parse(search_output)

    LOG.debug "Raw search results: #{search_results.length} conversations found"

    # Deduplicate URLs we already have
    existing_urls = @shared[:memory][:hits].map { |hit| hit[:url] }.to_set
    new_results = search_results.reject { |result| existing_urls.include?(result.dig("payload", "url")) }

    LOG.info "Found #{new_results.length} new conversations after deduplication"

    if new_results.empty?
      LOG.info "No new results found - all were duplicates"
      return []
    end

    LOG.debug do
      result_details = new_results.map.with_index do |result, i|
        "  #{i + 1}. URL: #{result.dig('payload', 'url')}\n" \
        "     Score: #{result['score']}\n" \
        "     Summary: #{result.dig('payload', 'summary')&.slice(0, 100)}..."
      end.join("\n\n")
      "New results found:\n#{result_details}"
    end

    # Fetch detailed data for new results
    new_enriched = []
    new_results.each_with_index do |result, i|
      url = result.dig("payload", "url")
      next unless url

      LOG.debug "Fetching details for new result #{i + 1}/#{new_results.length}: #{url}"

      begin
        fetch_cmd = "#{@shared[:script_dir]}/fetch-github-conversation #{Shellwords.escape(url)}"
        conversation_json = run_cmd(fetch_cmd)
        conversation_data = JSON.parse(conversation_json)

        metadata = extract_conversation_metadata(conversation_data)

        LOG.debug do
          "✓ Successfully fetched: #{metadata[:title]}\n" \
          "  Type: #{metadata[:type]}\n" \
          "  State: #{metadata[:state]}\n" \
          "  Comments: #{metadata[:comments_count]}"
        end

        new_enriched << {
          url: url,
          summary: result.dig("payload", "summary") || "",
          score: result["score"],
          conversation: conversation_data
        }
      rescue => e
        LOG.warn "Failed to fetch #{url}: #{e.message}"
      end
    end

    LOG.info "Successfully enriched #{new_enriched.length}/#{new_results.length} new conversations"
    new_enriched
  end

  def post(shared, prep_res, exec_res)
    return "final" if prep_res.nil? # Max depth reached

    # Add new findings to memory
    shared[:memory][:hits].concat(exec_res)
    shared[:memory][:search_queries] << prep_res

    # Add research notes
    if exec_res.any?
      notes = exec_res.map { |hit| "#{hit[:url]}: #{hit[:summary]}" }.join("\n")
      shared[:memory][:notes] << "Research iteration: #{notes}"
      LOG.info "Added #{exec_res.length} new conversations to memory"
    else
      LOG.info "No new conversations added this iteration"
    end

    # Increment depth
    shared[:current_depth] = (shared[:current_depth] || 0) + 1

    LOG.debug do
      "Current memory state:\n" \
      "  Total conversations: #{shared[:memory][:hits].length}\n" \
      "  Total search queries: #{shared[:memory][:search_queries].length}\n" \
      "  Current depth: #{shared[:current_depth]}/#{shared[:max_depth]}"
    end

    # Continue if under max depth, otherwise go to final report
    if shared[:current_depth] < shared[:max_depth] && exec_res.any?
      LOG.info "Continuing to next deep research iteration..."
      "default" # Continue deep research loop
    else
      LOG.info "Deep research complete, moving to final report..."
      "final"
    end
  end
end

class FinalReportNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    LOG.info "=== FINAL REPORT PHASE ==="
    LOG.info "Generating final report from all gathered data..."

    LOG.info "Research summary: #{shared[:memory][:hits].length} conversations analyzed, #{shared[:memory][:search_queries].length} queries used, #{shared[:current_depth] || 0} deep research iterations"

    LOG.debug do
      sources_list = shared[:memory][:hits].map.with_index do |hit, i|
        "  #{i + 1}. #{hit[:url]} (score: #{hit[:score]})"
      end.join("\n")
      "All conversation sources:\n#{sources_list}"
    end

    # Compile all findings
    all_findings = shared[:memory][:hits].map do |hit|
      <<~FINDING
      **Source**: #{hit[:url]}
      **Summary**: #{hit[:summary]}
      **Relevance Score**: #{hit[:score]}

      **Conversation Details**:
      #{JSON.pretty_generate(hit[:conversation])}
      FINDING
    end.join("\n\n---\n\n")

    prompt = fill_template(FINAL_REPORT_PROMPT, {
      question: shared[:question],
      clarifications: shared[:clarifications] || "None provided",
      all_findings: all_findings
    })

    LOG.debug "Calling LLM to generate final report..."
    prompt
  end

  def exec(prompt)
    call_llm(prompt, @shared[:llm_model])
  end

  def post(shared, prep_res, exec_res)
    LOG.info "=== FINAL REPORT ==="
    puts exec_res
    LOG.info "✓ Research complete! Total conversations analyzed: #{shared[:memory][:hits].length}"

    "default"
  end
end

# === Main Script ===

# Parse command-line options
options = {
  collection: nil,
  limit: 10,
  max_depth: 10,
  editor_file: nil,
  verbose: false,
  llm_model: ENV["LLM_MODEL"]
}

opt_parser = OptionParser.new do |opts|
  opts.banner = "Usage: #{File.basename($0)} \"QUESTION\" --collection COLLECTION [options]"

  opts.on("--collection NAME", "Qdrant collection name (required)") do |v|
    options[:collection] = v
  end

  opts.on("-n", "--limit N", Integer, "Max results per search (default: 10)") do |v|
    options[:limit] = v
  end

  opts.on("--max-depth N", Integer, "Max deep-research passes (default: 10)") do |v|
    options[:max_depth] = v
  end

  opts.on("--editor-file PATH", "Use fixed file instead of Tempfile") do |v|
    options[:editor_file] = v
  end

  opts.on("--verbose", "Show debug logs") do
    options[:verbose] = true
  end

  opts.on("--llm-model MODEL", "LLM model to use") do |v|
    options[:llm_model] = v
  end

  opts.on("-h", "--help", "Show this help message") do
    puts opts
    exit
  end
end

begin
  opt_parser.parse!
rescue OptionParser::InvalidOption => e
  abort "#{e.message}\n\n#{opt_parser}"
end

# Set logger level based on verbose flag
LOG.level = options[:verbose] ? Logger::DEBUG : Logger::INFO

# Validate required arguments
if ARGV.empty?
  abort opt_parser.to_s
end

question = ARGV.join(" ")

if question.strip.empty?
  abort "Error: Empty question provided"
end

unless options[:collection]
  abort "Error: --collection is required\n\n#{opt_parser}"
end

# Set up shared context
script_dir = File.expand_path(File.dirname(__FILE__))
shared = {
  question: question,
  collection: options[:collection],
  top_k: options[:limit],
  max_depth: options[:max_depth],
  editor_file: options[:editor_file],
  verbose: options[:verbose],
  llm_model: options[:llm_model],
  script_dir: script_dir
}

# Build the workflow
initial_node = InitialResearchNode.new
clarify_node = AskClarifyingNode.new
deep_node = DeepResearchNode.new
final_node = FinalReportNode.new

# Link the nodes
initial_node.next(clarify_node)
clarify_node.next(deep_node)
deep_node.next(deep_node) # Loop back to itself
deep_node.on("final", final_node)

# Create and run the flow
flow = Pocketflow::Flow.new(initial_node)

begin
  LOG.info "=== GITHUB CONVERSATIONS RESEARCH AGENT ==="
  LOG.info "Question: #{question}"
  LOG.info "Collection: #{options[:collection]}"
  LOG.info "Max results per search: #{options[:limit]}"
  LOG.info "Max deep research iterations: #{options[:max_depth]}"
  LOG.info "LLM model: #{options[:llm_model] || 'default'}"

  flow.run(shared)
rescue Interrupt
  LOG.error "\nResearch interrupted by user"
  exit 1
rescue => e
  LOG.error "Error: #{e.message}"
  LOG.debug e.backtrace.join("\n") if shared[:verbose]
  exit 1
end
