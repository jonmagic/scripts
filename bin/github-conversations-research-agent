#!/usr/bin/env ruby

# bin/github-conversations-research-agent: Multi-turn research agent for GitHub conversations
#
# This script implements a research workflow that:
# 1. Takes a natural language request and performs initial semantic search
# 2. Generates clarifying questions and opens them in $EDITOR
# 3. Performs iterative deep research based on clarifications
# 4. Produces a final well-formatted Markdown report citing all sources
#
# Usage: github-conversations-research-agent "REQUEST" --collection COLLECTION [options]
#
# Options:
#   --collection NAME        Qdrant collection name (required)
#   -n, --limit N           Max results per search (default: 10)
#   --max-depth N           Max deep-research passes (default: 10)
#   --editor-file PATH      Use fixed file instead of Tempfile
#   --clarifying-qa PATH    Path to file with clarifying Q&A to bypass interactive step
#   --verbose               Show debug logs
#   --fast-model MODEL      Fast LLM model for light reasoning
#   --reasoning-model MODEL Reasoning LLM model for complex analysis
#   --search-mode MODE      Override search mode (semantic, keyword, or hybrid - default: hybrid)
#   --cache-path PATH       Root path for caching fetched data
#
# The script uses the existing bin/semantic-search-github-conversations and
# bin/fetch-github-conversation scripts to gather context, and integrates
# with the llm CLI for AI-powered analysis.

require "json"
require "logger"
require "open3"
require "optparse"
require "tempfile"
require "shellwords"

# Load vendored Pocketflow library
require_relative "../lib/pocketflow"

# Load GitHub Deep Research Agent modules
require_relative "../lib/utils"
require_relative "../lib/github_deep_research_agent/end_node"

# Set up global logger
LOG = Logger.new($stdout)
LOG.level = Logger::INFO  # Default level, will be changed to DEBUG with --verbose

# === Embedded Prompt Templates ===

ASK_CLARIFY_PROMPT = <<~PROMPT
You are an expert analyst reviewing a research request and initial findings from GitHub conversations.

## Research Request
{{request}}

## Initial Findings Summary
{{initial_findings}}

Based on the question and initial findings, generate up to 4 clarifying questions that would help you better understand the intent of the request, bridge gaps in context to better refine the search (e.g. specifying the search space like a github organization or repository), and understand the expected output format (executive summary, detailed analysis, ADR, etc). If any of these areas are covered in their request or initial findings, do not ask about them.

Format your response as a numbered list with clear, specific questions. Each question should be on its own line starting with a number. The instructions should ask for inline answers to these questions.
PROMPT

SEMANTIC_RESEARCH_PROMPT = <<~PROMPT
You are an expert researcher mining GitHub conversations for actionable evidence.

## Original Request
{{request}}

## User Clarifications
{{clarifications}}

## Findings So Far
{{findings_summary}}

## Prior Search Queries
{{previous_queries}}

**Goal**
Identify the most significant information gaps and craft one new natural-language search query (≤ 2 sentences, no operators) that will surface GitHub conversations to close them.
Gaps may include — but are not limited to — missing implementation details, unclear project status (Implemented · In Progress · Proposed · Abandoned · Unknown), decisions, trade-offs, or alternative solutions.

When project "doneness" is uncertain, bias your query toward finding evidence that proves or disproves implementation (e.g. merged PRs, release notes, deployment comments). Otherwise, target whatever gap is highest-impact.

If helpful, suggest date ranges in ISO8601 format (created_after / created_before) to narrow the search.
If appropriate, specify ordering (order_by: created_at asc|desc).

Return a JSON object with:
- "query": the natural language search query
- "created_after": ISO date string (optional)
- "created_before": ISO date string (optional)
- "order_by": "created_at asc" or "created_at desc" (optional)

Do not use markdown code blocks - return only the raw JSON object.

*Examples*
{"query": "Confirmation that client-side rate limiting is merged and live", "created_after": "2024-01-01"}
{"query": "Alternative approaches to large-table migration performance", "order_by": "created_at desc"}
{"query": "Discussion showing why the authentication redesign was abandoned"}
PROMPT

GITHUB_SEARCH_PROMPT = <<~PROMPT
You are GitHub's top search power-user.

## Research request
{{request}}

## Known clarifications
{{clarifications}}

Return **one** GitHub search string that is likely to surface the best discussions.
• ≤ 5 terms/operators.
• Prefer operators when obvious (`repo:`, `author:`, `label:`, `is:`, `created:`, `updated:`).
• Otherwise fall back to 2-3 strong keywords.
Output only the search string, nothing else.
PROMPT

FINAL_REPORT_PROMPT = <<~PROMPT
You are an expert analyst preparing a comprehensive Markdown report.

## Original Request
{{request}}

## User Clarifications
{{clarifications}}

## Research Corpus
{{all_findings}}

Produce a well-structured Markdown report based on the initial request and clarifications and cite relevant sources used to support your findings.

**Style guide**

* Use proper Markdown headings (`##`, `###`) and omit horizontal rules (`---`).
* Every factual claim must be backed by an inline citation (full URL).
* If status cannot be confirmed, mark it **Unknown** and note what evidence is missing.

Return only the Markdown document—no extra commentary.
PROMPT

EXTRACT_CLAIMS_PROMPT = <<~PROMPT
You are an expert fact-checker analyzing a research report for factual claims.

## Research Report
{{report}}

Extract all distinct factual claims from this report. Focus on specific, verifiable statements about:
- Implementation status (what is implemented, merged, released, etc.)
- Technical decisions and their outcomes
- Project statuses and timelines
- Performance metrics and measurements
- Process changes and their effects

**Requirements:**
- Each claim should be one clear, specific sentence
- Focus on claims that can be verified against GitHub conversations
- Ignore general opinions, recommendations, or conclusions
- Limit to the first 25 most important claims
- Do not include claims about what the report itself says or concludes

Return your response as a JSON array of strings, where each string is one factual claim. Do not use markdown code blocks - return only the raw JSON array.

Example format:
["Feature X was merged in PR #123", "Performance improved by 50% after the optimization", "The team decided to use library Y instead of Z"]
PROMPT

VERIFY_CLAIM_PROMPT = <<~PROMPT
You are an expert fact-checker determining if evidence supports a specific claim.

## Claim to Verify
{{claim}}

## Evidence from GitHub Conversations
{{evidence}}

Does the evidence clearly support this claim? Consider:
- Is there direct evidence that confirms the claim?
- Do the conversations provide sufficient detail to verify the claim?
- Are there any contradictions in the evidence?

Respond with exactly one word:
- "SUPPORTED" if the evidence clearly supports the claim
- "UNSUPPORTED" if the evidence does not support the claim or is insufficient

Do not provide any explanation - just the single word response.
PROMPT

UNSUPPORTED_CLAIMS_RESEARCH_PROMPT = <<~PROMPT
You are an expert researcher focusing on verifying unsupported claims from a research report.

## Original Request
{{request}}

## User Clarifications
{{clarifications}}

## Unsupported Claims That Need Verification
{{unsupported_claims}}

## Previous Findings
{{findings_summary}}

## Prior Search Queries
{{previous_queries}}

**Goal**
Generate a natural-language search query (≤ 2 sentences, no operators) specifically designed to find GitHub conversations that could provide evidence for the unsupported claims listed above.

Focus on finding conversations that contain:
- Direct evidence of implementation status
- Specific technical decisions and outcomes
- Concrete examples or proof points
- Timeline information and project updates

Return a JSON object with:
- "query": the natural language search query targeting evidence for unsupported claims
- "created_after": ISO date string (optional)
- "created_before": ISO date string (optional)
- "order_by": "created_at asc" or "created_at desc" (optional)

Do not use markdown code blocks - return only the raw JSON object.

*Examples*
{"query": "Evidence of feature implementation and merge status with specific PR numbers", "order_by": "created_at desc"}
{"query": "Performance measurement results and optimization outcomes with concrete metrics"}
PROMPT

# === Helper Methods ===

# Public: Runs a shell command and returns stdout. Aborts if the command fails.
#
# cmd - The shell command to run (String).
#
# Returns the standard output of the command (String).
# Raises SystemExit if the command fails.
def run_cmd(cmd)
  stdout, stderr, status = Open3.capture3(cmd)
  abort "Command failed: #{cmd}\n#{stderr}" unless status.success?
  stdout.strip
end

# Public: Runs a shell command and returns stdout. Raises exception if the command fails.
#
# cmd - The shell command to run (String).
#
# Returns the standard output of the command (String).
# Raises RuntimeError if the command fails.
def run_cmd_safe(cmd)
  stdout, stderr, status = Open3.capture3(cmd)
  raise "Command failed: #{cmd}\n#{stderr}" unless status.success?
  stdout.strip
end

# Public: Checks if a required command-line dependency is available in PATH.
#
# cmd - The String name of the command to check.
#
# Returns nothing. Exits if not found.
def check_dependency(cmd)
  system("which #{cmd} > /dev/null 2>&1") || abort("Required dependency '#{cmd}' not found in PATH.")
end

# Public: Gets the editor using Git's resolution order.
#
# Returns the editor command as a String.
def get_git_editor
  # Follow Git's editor resolution order:
  # 1. GIT_EDITOR environment variable
  # 2. core.editor config value
  # 3. VISUAL environment variable
  # 4. EDITOR environment variable
  # 5. Fall back to system default

  return ENV["GIT_EDITOR"] if ENV["GIT_EDITOR"] && !ENV["GIT_EDITOR"].strip.empty?

  # Try git config core.editor
  git_config_editor = `git config --get core.editor 2>/dev/null`.strip
  return git_config_editor unless git_config_editor.empty?

  return ENV["VISUAL"] if ENV["VISUAL"] && !ENV["VISUAL"].strip.empty?
  return ENV["EDITOR"] if ENV["EDITOR"] && !ENV["EDITOR"].strip.empty?

  # Fall back to nano as a sensible default
  "nano"
end

# Public: Opens a text editor with the given content and returns the edited result.
#
# text - The String text to edit.
# file_path - Optional String path to use instead of a temporary file.
#
# Returns the edited text as a String.
def edit_text(text, file_path = nil)
  if file_path
    File.write(file_path, text)
    tmp_path = file_path
  else
    tmp = Tempfile.create(["research_edit", ".md"])
    tmp.puts text
    tmp.flush
    tmp_path = tmp.path
  end

  # Get editor using Git's resolution order
  editor = get_git_editor()

  # Open editor
  unless system("#{editor} #{tmp_path}")
    abort "Editor command failed: #{editor}"
  end

  # Read the edited content
  File.read(tmp_path)
ensure
  tmp&.close unless file_path
end

# Public: Fills in template variables in a prompt string.
#
# template - The String template with {{variable}} placeholders.
# variables - Hash of variable names to values.
#
# Returns the filled template as a String.
def fill_template(template, variables)
  result = template.dup
  variables.each do |key, value|
    result.gsub!("{{#{key}}}", value.to_s)
  end
  result
end

# Public: Calls the LLM CLI for chat completion.
#
# prompt - The String prompt to send.
# model - Optional String model name.
#
# Returns the LLM response as a String.
def call_llm(prompt, model = nil)
  check_dependency("llm") # Check only when needed
  model_flag = model ? "-m #{Shellwords.escape(model)}" : ""

  # Use a temporary file to avoid "Argument list too long" errors
  Tempfile.create(["llm_prompt", ".txt"]) do |tmpfile|
    tmpfile.write(prompt)
    tmpfile.flush
    cmd = "llm #{model_flag} < #{Shellwords.escape(tmpfile.path)}"
    run_cmd_safe(cmd)
  end
end

# Public: Detects if an error message indicates context is too large for the model.
#
# error_message - The String error message to check.
#
# Returns true if the error indicates context is too large, false otherwise.
def context_too_large_error?(error_message)
  # Azure OpenAI error patterns
  return true if error_message.include?("maximum context length")
  return true if error_message.include?("token limit")
  return true if error_message.include?("Request too large")
  return true if error_message.include?("context_length_exceeded")

  # OpenAI API error patterns
  return true if error_message.include?("This model's maximum context length")
  return true if error_message.include?("reduce the length of the messages")

  # Anthropic Claude error patterns
  return true if error_message.include?("max_tokens")
  return true if error_message.include?("context window")

  # Generic patterns
  return true if error_message.include?("too long")
  return true if error_message.include?("exceeds")

  false
end

# Public: Detects if an error message indicates a rate limit that should trigger compaction.
#
# error_message - The String error message to check.
#
# Returns true if the error indicates rate limiting, false otherwise.
def rate_limit_error?(error_message)
  # Azure OpenAI rate limit patterns
  return true if error_message.include?("rate limit")
  return true if error_message.include?("Error code: 429")
  return true if error_message.include?("exceeded token rate limit")

  # OpenAI API rate limit patterns
  return true if error_message.include?("Rate limit reached")
  return true if error_message.include?("Too Many Requests")

  # Anthropic Claude rate limit patterns
  return true if error_message.include?("rate_limit_error")
  return true if error_message.include?("overloaded_error")

  false
end

# Public: Sorts conversations by priority for context compaction.
#
# This implements a composite scoring strategy that prioritizes:
# 1. Conversations with non-empty summaries (+10 points)
# 2. Higher relevance scores (when available)
# 3. Conversations found in earlier iterations (recency bonus)
#
# hits - Array of conversation hashes with :summary, :score, :url keys
#
# Returns the sorted array (modifies in place).
def sort_conversations_by_priority!(hits)
  hits.sort_by! do |hit|
    composite_score = 0

    # Strategy 1: Prioritize hits with summaries (+10 points)
    unless hit[:summary].to_s.strip.empty?
      composite_score += 10
    end

    # Strategy 2: Use relevance score when available
    score = hit[:score] || 0
    if score > 0
      composite_score += score
    end

    # Strategy 3: Slight bonus for earlier discoveries (more context)
    # Assume lower array index = found earlier
    iteration_bonus = hits.length - hits.index(hit)
    composite_score += (iteration_bonus * 0.1)

    # Sort in descending order (highest priority first)
    -composite_score
  end

  hits
end

# Public: Extracts conversation metadata from GitHub conversation data.
#
# conversation_data - The parsed JSON from fetch-github-conversation.
#
# Returns a Hash with conversation metadata and logging information.
def extract_conversation_metadata(conversation_data)
  # Determine type and extract conversation metadata
  conversation_type = if conversation_data["issue"]
    "issue"
  elsif conversation_data["pr"]
    "pull request"
  elsif conversation_data["discussion"]
    "discussion"
  else
    "unknown"
  end

  # Get the actual conversation object based on type
  conversation_obj = conversation_data["issue"] || conversation_data["pr"] || conversation_data["discussion"] || {}

  # Extract metadata for logging and return
  {
    type: conversation_type,
    title: conversation_obj["title"] || "Unknown title",
    state: conversation_obj["state"] || "unknown",
    comments_count: conversation_data["comments"]&.length || 0
  }
end

# Public: Parses structured semantic search response from LLM.
#
# llm_response - The String response from LLM (should be JSON).
#
# Returns a Hash with parsed query and filter information.
def parse_semantic_search_response(llm_response)
  begin
    # Clean up response - remove markdown code blocks if present
    cleaned_response = llm_response.strip
    if cleaned_response.start_with?('```json')
      # Remove ```json from start and ``` from end
      cleaned_response = cleaned_response.gsub(/\A```json\s*/, '').gsub(/\s*```\z/, '')
    elsif cleaned_response.start_with?('```')
      # Remove generic ``` from start and end
      cleaned_response = cleaned_response.gsub(/\A```\s*/, '').gsub(/\s*```\z/, '')
    end

    # Try to parse as JSON first
    parsed = JSON.parse(cleaned_response.strip)

    # Validate required fields
    unless parsed["query"] && parsed["query"].is_a?(String)
      raise "Missing or invalid 'query' field"
    end

    result = { query: parsed["query"] }

    # Add optional fields if present
    if parsed["created_after"]
      result[:created_after] = parsed["created_after"]
    end

    if parsed["created_before"]
      result[:created_before] = parsed["created_before"]
    end

    if parsed["order_by"]
      # Parse order_by into field and direction
      parts = parsed["order_by"].split(" ", 2)
      if parts.length == 2 && parts[0] == "created_at" && %w[asc desc].include?(parts[1])
        result[:order_by] = { key: parts[0], direction: parts[1] }
      else
        LOG.warn "Invalid order_by format: #{parsed['order_by']}, ignoring"
      end
    end

    result
  rescue JSON::ParserError
    # Fallback: treat as plain text query for backwards compatibility
    LOG.warn "LLM response was not valid JSON, treating as plain text query"
    LOG.debug "Raw LLM response: #{llm_response.inspect}"
    { query: llm_response.strip }
  rescue => e
    LOG.warn "Error parsing structured search response: #{e.message}, treating as plain text"
    LOG.debug "Raw LLM response: #{llm_response.inspect}" if llm_response
    { query: llm_response.strip }
  end
end


# Public: Extracts qualifiers from user query and builds semantic search query.
#
# user_query - String containing the user's query with potential qualifiers.
#
# Returns a Hash with:
#   - :semantic_query - String with qualifiers stripped for embedding
#   - :repo_filter - String with repo qualifier (e.g., "owner/name") or nil
#   - :author_filter - String with author qualifier or nil
def build_semantic_query(user_query)
  # Extract repo: and author: qualifiers
  repo_match = user_query.match(/\brepo:(\S+)/)
  author_match = user_query.match(/\bauthor:(\S+)/)

  # Strip qualifiers from the query for semantic search
  semantic_query = user_query.dup
  semantic_query.gsub!(/\brepo:\S+/, '')
  semantic_query.gsub!(/\bauthor:\S+/, '')
  semantic_query.strip!

  # Clean up extra whitespace
  semantic_query.gsub!(/\s+/, ' ')

  {
    semantic_query: semantic_query,
    repo_filter: repo_match ? repo_match[1] : nil,
    author_filter: author_match ? author_match[1] : nil
  }
end

# Public: Fetches existing summary from Qdrant using URL filter.
#
# url - The String GitHub conversation URL to search for.
# collection - The String Qdrant collection name.
# script_dir - The String path to the script directory.
#
# Returns the summary String if found, nil otherwise.
def fetch_summary_from_qdrant(url, collection, script_dir)
  begin
    search_cmd = "#{script_dir}/semantic-search-github-conversations"
    search_cmd += " --collection #{Shellwords.escape(collection)}"
    search_cmd += " --filter url:#{Shellwords.escape(url)}"
    search_cmd += " --limit 1"
    search_cmd += " --format json"
    search_cmd += " #{Shellwords.escape('*')}"  # Dummy query since we're filtering by URL

    LOG.debug "Fetching existing summary from Qdrant: #{search_cmd}"

    search_output = run_cmd(search_cmd)
    search_results = JSON.parse(search_output)

    if search_results.any?
      summary = search_results.first.dig("payload", "summary")
      LOG.debug "Found existing summary in Qdrant for #{url}: #{summary&.slice(0, 100)}..."
      return summary
    else
      LOG.debug "No existing summary found in Qdrant for #{url}"
      return nil
    end
  rescue => e
    LOG.warn "Failed to fetch summary from Qdrant for #{url}: #{e.message}"
    return nil
  end
end

# Public: Generates a new summary for a GitHub conversation.
#
# url - The String GitHub conversation URL.
# executive_summary_prompt_path - The String path to the prompt file.
# cache_path - The String cache path (optional).
#
# Returns the generated summary String, or empty string on failure.
def generate_new_summary(url, executive_summary_prompt_path, cache_path = nil)
  begin
    LOG.debug "Generating new summary for #{url}"

    summarize_cmd = "#{File.dirname(__FILE__)}/summarize-github-conversation"
    summarize_cmd += " --executive-summary-prompt-path #{Shellwords.escape(executive_summary_prompt_path)}"

    if cache_path
      summarize_cmd += " --cache-path #{Shellwords.escape(cache_path)}"
    end

    summarize_cmd += " #{Shellwords.escape(url)}"

    LOG.debug "Running summary generation: #{summarize_cmd}"

    summary = run_cmd(summarize_cmd)
    LOG.debug "Generated summary for #{url}: #{summary&.slice(0, 100)}..."

    return summary.strip
  rescue => e
    LOG.warn "Failed to generate summary for #{url}: #{e.message}"
    return ""
  end
end

# Public: Gets or generates a summary for a GitHub conversation.
#
# url - The String GitHub conversation URL.
# collection - The String Qdrant collection name.
# script_dir - The String path to the script directory.
# executive_summary_prompt_path - The String path to the prompt file (optional).
# cache_path - The String cache path (optional).
#
# Returns the summary String (empty string if generation fails or is disabled).
def get_or_generate_summary(url, collection, script_dir, executive_summary_prompt_path = nil, cache_path = nil)
  # Step 1: Try to fetch existing summary from Qdrant
  existing_summary = fetch_summary_from_qdrant(url, collection, script_dir)
  return existing_summary if existing_summary && !existing_summary.empty?

  # Step 2: Generate new summary if prompt path is provided
  if executive_summary_prompt_path
    return generate_new_summary(url, executive_summary_prompt_path, cache_path)
  else
    LOG.warn "No executive summary prompt path provided - skipping summary generation for #{url}"
    return ""
  end
end

# Public: Builds semantic search command with filters and ordering.
#
# search_plan - Hash containing query and optional filters.
# script_dir - String path to the script directory.
# collection - String collection name.
# top_k - Integer limit for results.
#
# Returns the command string.
def build_semantic_search_command(search_plan, script_dir, collection, top_k)
  cmd = "#{script_dir}/semantic-search-github-conversations"
  cmd += " #{Shellwords.escape(search_plan[:semantic_query] || search_plan[:query])}"
  cmd += " --collection #{Shellwords.escape(collection)}"
  cmd += " --limit #{top_k}"
  cmd += " --format json"

  # Add date filters if present
  if search_plan[:created_after]
    cmd += " --filter created_after:#{Shellwords.escape(search_plan[:created_after])}"
  end
  if search_plan[:created_before]
    cmd += " --filter created_before:#{Shellwords.escape(search_plan[:created_before])}"
  end

  # Add repo filter if present
  if search_plan[:repo_filter]
    cmd += " --filter repo:#{Shellwords.escape(search_plan[:repo_filter])}"
  end

  # Add author filter if present
  if search_plan[:author_filter]
    cmd += " --filter author:#{Shellwords.escape(search_plan[:author_filter])}"
  end

  # Add ordering if present
  if search_plan[:order_by]
    order_by_str = "#{search_plan[:order_by][:key]} #{search_plan[:order_by][:direction]}"
    cmd += " --order-by #{Shellwords.escape(order_by_str)}"
  end

  cmd
end

# Public: Extracts factual claims from a research report using LLM.
#
# report - The String research report to extract claims from.
# model - Optional String model name for LLM.
#
# Returns an Array of claim strings.
def extract_claims_from_report(report, model = nil)
  prompt = fill_template(EXTRACT_CLAIMS_PROMPT, { report: report })

  LOG.debug "Calling LLM to extract claims from report..."

  begin
    llm_response = call_llm(prompt, model)

    # Clean up response - remove markdown code blocks if present
    cleaned_response = llm_response.strip
    if cleaned_response.start_with?('```json')
      # Remove ```json from start and ``` from end
      cleaned_response = cleaned_response.gsub(/\A```json\s*/, '').gsub(/\s*```\z/, '')
    elsif cleaned_response.start_with?('```')
      # Remove generic ``` from start and end
      cleaned_response = cleaned_response.gsub(/\A```\s*/, '').gsub(/\s*```\z/, '')
    end

    claims = JSON.parse(cleaned_response.strip)

    unless claims.is_a?(Array)
      LOG.warn "LLM response was not an array, treating as empty claims list"
      return []
    end

    # Limit to first 25 claims as specified
    claims = claims.first(25)

    LOG.debug "Extracted #{claims.length} claims from report"
    claims
  rescue JSON::ParserError => e
    LOG.warn "Failed to parse claims as JSON: #{e.message}"
    LOG.debug "Raw LLM response: #{llm_response.inspect}" if llm_response
    []
  rescue => e
    LOG.warn "Failed to extract claims: #{e.message}"
    []
  end
end

# Public: Verifies a single claim against evidence using LLM.
#
# claim - The String claim to verify.
# evidence - The String evidence to check against.
# model - Optional String model name for LLM.
#
# Returns true if claim is supported, false otherwise.
def verify_claim_against_evidence(claim, evidence, model = nil)
  prompt = fill_template(VERIFY_CLAIM_PROMPT, {
    claim: claim,
    evidence: evidence
  })

  LOG.debug "Verifying claim: #{claim.slice(0, 100)}..."

  begin
    llm_response = call_llm(prompt, model)
    result = llm_response.strip.upcase

    case result
    when "SUPPORTED"
      LOG.debug "✓ Claim supported"
      true
    when "UNSUPPORTED"
      LOG.debug "✗ Claim unsupported"
      false
    else
      LOG.warn "Unexpected verification response: '#{result}', treating as unsupported"
      false
    end
  rescue => e
    LOG.warn "Failed to verify claim: #{e.message}, treating as unsupported"
    false
  end
end

# Public: Searches for evidence related to a specific claim.
#
# claim - The String claim to search evidence for.
# collection - The String Qdrant collection name.
# script_dir - The String path to the script directory.
# limit - The Integer number of results to return (default: 3).
#
# Returns a String containing the summarized evidence.
def search_evidence_for_claim(claim, collection, script_dir, limit = 3)
  search_cmd = "#{script_dir}/semantic-search-github-conversations"
  search_cmd += " #{Shellwords.escape(claim)}"
  search_cmd += " --collection #{Shellwords.escape(collection)}"
  search_cmd += " --limit #{limit}"
  search_cmd += " --format json"

  LOG.debug "Searching for evidence: #{search_cmd}"

  begin
    search_output = run_cmd_safe(search_cmd)
    search_results = JSON.parse(search_output)

    if search_results.empty?
      LOG.debug "No evidence found for claim"
      return "No relevant evidence found."
    end

    # Extract summaries from search results
    evidence_parts = search_results.map.with_index do |result, i|
      url = result.dig("payload", "url") || "Unknown URL"
      summary = result.dig("payload", "summary") || "No summary available"
      score = result["score"] || 0.0

      "Evidence #{i + 1} (Score: #{score.round(3)}):\nSource: #{url}\nSummary: #{summary}"
    end

    evidence = evidence_parts.join("\n\n---\n\n")
    LOG.debug "Found #{search_results.length} pieces of evidence for claim"

    evidence
  rescue => e
    LOG.warn "Failed to search for evidence: #{e.message}"
    "Error retrieving evidence: #{e.message}"
  end
end

# === Pocketflow Nodes ===

class InitialResearchNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context for use in exec
    LOG.info "=== INITIAL RESEARCH PHASE ==="
    LOG.info "Starting initial semantic search for: #{shared[:request]}"
    LOG.debug "Collection: #{shared[:collection]}"
    LOG.debug "Max results: #{shared[:top_k]}"

    request = shared[:request]
    collection = shared[:collection]
    top_k = shared[:top_k]
    script_dir = shared[:script_dir]

    # Extract qualifiers from the initial request for semantic search
    semantic_query_info = build_semantic_query(request)
    LOG.debug "Extracted semantic query: '#{semantic_query_info[:semantic_query]}'"
    LOG.debug "Extracted repo filter: #{semantic_query_info[:repo_filter]}" if semantic_query_info[:repo_filter]
    LOG.debug "Extracted author filter: #{semantic_query_info[:author_filter]}" if semantic_query_info[:author_filter]

    # Build search plan with extracted qualifiers
    search_plan = semantic_query_info.merge({ query: request })

    # Run semantic search with qualifier extraction
    search_cmd = build_semantic_search_command(search_plan, script_dir, collection, top_k)
    LOG.debug "Running search command: #{search_cmd}"

    search_output = run_cmd(search_cmd)
    search_results = JSON.parse(search_output)

    LOG.info "Found #{search_results.length} initial results"
    # LOG.debug example: showing detailed search results when verbose logging is enabled
    LOG.debug do
      result_details = search_results.map.with_index do |result, i|
        "  #{i + 1}. URL: #{result.dig('payload', 'url')}\n" \
        "     Score: #{result['score']}\n" \
        "     Summary: #{result.dig('payload', 'summary')&.slice(0, 100)}..."
      end.join("\n\n")
      "Initial search results:\n#{result_details}"
    end

    search_results
  end

  def exec(search_results)
    LOG.info "Fetching detailed conversation data for #{search_results.length} results..."

    # Fetch detailed conversation data for each result
    enriched_results = []

    search_results.each_with_index do |result, i|
      url = result.dig("payload", "url")
      next unless url

      LOG.debug "Fetching details for result #{i + 1}/#{search_results.length}: #{url}"

      begin
        fetch_cmd = "#{@shared[:script_dir]}/fetch-github-conversation"
        if @shared[:cache_path]
          fetch_cmd += " --cache-path #{Shellwords.escape(@shared[:cache_path])}"
        end
        fetch_cmd += " #{Shellwords.escape(url)}"

        conversation_json = run_cmd_safe(fetch_cmd)
        conversation_data = JSON.parse(conversation_json)

        metadata = extract_conversation_metadata(conversation_data)

        LOG.debug do
          "✓ Successfully fetched: #{metadata[:title]}\n" \
          "  Type: #{metadata[:type]}\n" \
          "  State: #{metadata[:state]}\n" \
          "  Comments: #{metadata[:comments_count]}"
        end

        enriched_results << {
          url: url,
          summary: result.dig("payload", "summary") || "",
          score: result["score"],
          conversation: conversation_data
        }
      rescue => e
        LOG.warn "Failed to fetch #{url}: #{e.message}"
      end
    end

    LOG.info "Successfully enriched #{enriched_results.length}/#{search_results.length} conversations"
    enriched_results
  end

  def post(shared, prep_res, exec_res)
    shared[:memory] ||= {}
    shared[:memory][:hits] = exec_res
    shared[:memory][:notes] = []
    shared[:memory][:search_queries] = [shared[:request]]

    LOG.info "✓ Initial research complete: #{exec_res.length} conversations collected"
    LOG.debug "Moving to clarifying questions phase..."

    nil
  end
end

class AskClarifyingNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    LOG.info "=== CLARIFYING QUESTIONS PHASE ==="
    LOG.info "Generating clarifying questions based on initial findings..."

    # Summarize initial findings
    initial_findings = shared[:memory][:hits].map do |hit|
      "- #{hit[:url]}: #{hit[:summary]}"
    end.join("\n")

    LOG.debug do
      "Initial findings summary:\n#{initial_findings}"
    end

    # Fill template and call LLM
    prompt = fill_template(ASK_CLARIFY_PROMPT, {
      request: shared[:request],
      initial_findings: initial_findings
    })

    LOG.debug "Calling LLM to generate clarifying questions..."
    # Use fast model for clarifying questions - this is light reasoning to generate questions based on initial findings
    llm_response = call_llm(prompt, shared[:models][:fast])

    LOG.info "Generated clarifying questions for user review"
    LOG.debug do
      "Generated questions:\n#{'=' * 60}\n#{llm_response}\n#{'=' * 60}"
    end

    llm_response
  end

  def exec(clarifying_questions)
    # Check if we have a pre-written Q&A file to bypass interactive step
    if @shared[:clarifying_qa]
      LOG.info "Using pre-written clarifying Q&A from file: #{@shared[:clarifying_qa]}"

      unless File.exist?(@shared[:clarifying_qa])
        abort "Error: Clarifying Q&A file not found: #{@shared[:clarifying_qa]}"
      end

      edited_content = File.read(@shared[:clarifying_qa])
      LOG.debug do
        "Pre-written clarifications:\n#{'=' * 60}\n#{edited_content}\n#{'=' * 60}"
      end

      return edited_content
    end

    LOG.info "Opening editor for user to answer clarifying questions..."

    # Prepare editor content
    editor_content = <<~CONTENT
Please review the following questions and provide inline answers to help focus the research:

#{clarifying_questions}
CONTENT

    # Open editor
    edited_content = edit_text(editor_content, @shared[:editor_file])

    LOG.info "User provided clarifications"
    LOG.debug do
      "User clarifications:\n#{'=' * 60}\n#{edited_content}\n#{'=' * 60}"
    end

    edited_content
  end

  def post(shared, prep_res, exec_res)
    shared[:clarifications] = exec_res
    LOG.info "✓ Clarifications collected, proceeding to planning phase"
    LOG.debug "Moving to planning phase..."

    nil
  end
end

# PlannerNode: Uses LLM to generate GitHub search queries based on the research request and clarifications.
#
# This node uses the GITHUB_SEARCH_PROMPT to generate targeted GitHub search strings:
# - Takes the research request and clarifications as input
# - Generates GitHub-compatible search strings with operators when appropriate
# - Defaults to keyword search since LLM produces GitHub search strings
# - Supports --search-mode flag to override default behavior
#
# The generated queries are optimized for GitHub's search syntax and can include
# operators like repo:, author:, label:, is:, created:, updated: when beneficial.
class PlannerNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    depth = shared[:current_depth] || 0
    max_depth = shared[:max_depth]

    LOG.info "=== PLANNING PHASE (Iteration #{depth + 1}/#{max_depth}) ==="

    # Check if we have unsupported claims to research
    if shared[:unsupported_claims] && shared[:unsupported_claims].any?
      LOG.info "Focusing search on gathering evidence for #{shared[:unsupported_claims].length} unsupported claims"

      # Use special prompt for unsupported claims research
      unsupported_claims_list = shared[:unsupported_claims].map.with_index do |claim, i|
        "#{i + 1}. #{claim}"
      end.join("\n")

      findings_summary = shared[:memory][:notes].join("\n\n")
      previous_queries = shared[:memory][:search_queries].join(", ")

      prompt = fill_template(UNSUPPORTED_CLAIMS_RESEARCH_PROMPT, {
        request: shared[:request],
        clarifications: shared[:clarifications] || "",
        unsupported_claims: unsupported_claims_list,
        findings_summary: findings_summary,
        previous_queries: previous_queries
      })

      LOG.debug "Calling LLM to generate search query for unsupported claims..."
      llm_response = call_llm(prompt, shared[:models][:fast])
      refined_query = parse_semantic_search_response(llm_response)
      LOG.info "Generated claim verification search plan: #{refined_query}"

      return refined_query
    end

    LOG.info "Determining search strategy based on query analysis..."

    # Check if we've reached max depth
    if depth >= max_depth
      LOG.info "Maximum depth reached, moving to final report"
      return nil
    end

    # For the first iteration, we need to generate a different query than what was used in InitialResearchNode
    # to avoid duplicates. We'll use the appropriate search mode logic.
    if depth == 0
      LOG.info "First iteration - generating query different from initial research"
    end

    # Generate query based on search mode
    findings_summary = shared[:memory][:notes].join("\n\n")
    previous_queries = shared[:memory][:search_queries].join(", ")

    LOG.debug do
      "Current research context:\n" \
      "  Previous queries: #{previous_queries}\n" \
      "  Total conversations found so far: #{shared[:memory][:hits].length}\n" \
      "  Research notes accumulated: #{shared[:memory][:notes].length}"
    end

    search_mode = shared[:search_mode]

    if search_mode == "semantic"
      # Generate natural language query for semantic search
      prompt = fill_template(SEMANTIC_RESEARCH_PROMPT, {
        request: shared[:request],
        clarifications: shared[:clarifications] || "",
        findings_summary: findings_summary,
        previous_queries: previous_queries
      })

      LOG.debug "Calling LLM to generate natural language search query..."
      llm_response = call_llm(prompt, shared[:models][:fast])
      refined_query = parse_semantic_search_response(llm_response)
      LOG.info "Generated semantic search plan: #{refined_query}"
    elsif search_mode == "keyword"
      # Generate GitHub search string for keyword search
      prompt = fill_template(GITHUB_SEARCH_PROMPT, {
        request: shared[:request],
        clarifications: shared[:clarifications] || ""
      })

      LOG.debug "Calling LLM to generate GitHub search query..."
      refined_query = call_llm(prompt, shared[:models][:fast])
      LOG.info "Generated GitHub search query: \"#{refined_query}\""
    else
      # Hybrid mode: generate both semantic and keyword queries
      semantic_prompt = fill_template(SEMANTIC_RESEARCH_PROMPT, {
        request: shared[:request],
        clarifications: shared[:clarifications] || "",
        findings_summary: findings_summary,
        previous_queries: previous_queries
      })

      keyword_prompt = fill_template(GITHUB_SEARCH_PROMPT, {
        request: shared[:request],
        clarifications: shared[:clarifications] || ""
      })

      LOG.debug "Calling LLM to generate semantic query..."
      semantic_response = call_llm(semantic_prompt, shared[:models][:fast])
      semantic_query = parse_semantic_search_response(semantic_response)
      LOG.info "Generated semantic query: #{semantic_query}"

      LOG.debug "Calling LLM to generate keyword query..."
      keyword_query = call_llm(keyword_prompt, shared[:models][:fast])
      LOG.info "Generated keyword query: \"#{keyword_query}\""

      refined_query = {
        semantic: semantic_query,
        keyword: keyword_query
      }
    end

    refined_query
  end

  def exec(current_query)
    return nil if current_query.nil?

    # Store the current query for use by RetrieverNode
    @shared[:current_query] = current_query

    # Determine search mode based on flags and iteration depth
    search_mode = @shared[:search_mode]
    depth = @shared[:current_depth] || 0

    if search_mode == "semantic"
      tool = :semantic
      LOG.info "Forced semantic search mode via --search-mode flag"
    elsif search_mode == "keyword"
      tool = :keyword
      LOG.info "Forced keyword search mode via --search-mode flag"
    else
      # Hybrid mode: do both semantic and keyword searches
      tool = :hybrid
      LOG.info "Hybrid mode: Running both semantic and keyword searches"
    end

    # For hybrid mode, we need to check if we have separate semantic/keyword queries or a single query
    if tool == :hybrid
      # Check if current_query has separate semantic and keyword queries (from normal planning)
      # or if it's a single query (from unsupported claims research)
      if current_query.is_a?(Hash) && current_query[:semantic] && current_query[:keyword]
        # Normal hybrid mode with separate queries
        semantic_query = current_query[:semantic]
        search_plan = {
          tool: :hybrid,
          semantic_query: semantic_query[:query],
          keyword_query: current_query[:keyword]
        }

        # Add semantic search filters if present
        if semantic_query[:created_after]
          search_plan[:created_after] = semantic_query[:created_after]
        end
        if semantic_query[:created_before]
          search_plan[:created_before] = semantic_query[:created_before]
        end
        if semantic_query[:order_by]
          search_plan[:order_by] = semantic_query[:order_by]
        end
      else
        # Single query (likely from unsupported claims research) - treat as semantic search
        LOG.info "Single query provided, treating as semantic search in hybrid mode"
        tool = :semantic

        # Fall through to semantic search handling below
        search_plan = {
          tool: :semantic,
          query: current_query[:query] || current_query
        }

        # Add filters if present
        if current_query.is_a?(Hash)
          if current_query[:created_after]
            search_plan[:created_after] = current_query[:created_after]
          end
          if current_query[:created_before]
            search_plan[:created_before] = current_query[:created_before]
          end
          if current_query[:order_by]
            search_plan[:order_by] = current_query[:order_by]
          end
        end
      end
    end

    # Handle non-hybrid modes or fall-through from hybrid mode
    if tool != :hybrid
      # Handle both structured and legacy query formats
      if current_query.is_a?(Hash) && current_query[:query]
        # New structured format
        search_plan = {
          tool: tool,
          query: current_query[:query]
        }

        # Add filters if present
        if current_query[:created_after]
          search_plan[:created_after] = current_query[:created_after]
        end
        if current_query[:created_before]
          search_plan[:created_before] = current_query[:created_before]
        end
        if current_query[:order_by]
          search_plan[:order_by] = current_query[:order_by]
        end
      else
        # Legacy string format - extract potential qualifiers from the query for logging/UI
        qualifiers = {}
        if current_query.is_a?(String)
          %w[repo author label is created updated].each do |op|
            if current_query =~ /\b#{op}:(\S+)/i
              qualifiers[op.to_sym] = $1
              LOG.debug "Extracted #{op} qualifier: #{qualifiers[op.to_sym]}"
            end
          end
        end

        search_plan = {
          tool: tool,
          query: current_query,
          qualifiers: qualifiers
        }
      end
    end

    LOG.debug "Search plan: #{search_plan}"

    search_plan
  end

  def post(shared, prep_res, exec_res)
    return "final" if prep_res.nil? # Max depth reached

    # Store the search plan for RetrieverNode
    shared[:next_search] = exec_res

    LOG.info "✓ Planning complete, proceeding to retrieval"
    LOG.debug "Moving to retrieval phase..."

    nil
  end
end

# RetrieverNode: Executes the actual search based on PlannerNode's decisions.
#
# This node retrieves conversations using the search strategy determined by PlannerNode.
# Currently hard-coded to semantic search to maintain compatibility during transition.
# TODO: Implement keyword search support in next iteration.
class RetrieverNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    search_plan = shared[:next_search]

    if search_plan.nil?
      LOG.error "No search plan found from PlannerNode"
      return nil
    end

    LOG.info "=== RETRIEVAL PHASE ==="
    LOG.info "Executing #{search_plan[:tool]} search with query: \"#{search_plan[:query]}\""

    search_plan
  end

  def exec(search_plan)
    return [] if search_plan.nil?

    tool = search_plan[:tool]
    collection = @shared[:collection]
    top_k = @shared[:top_k]
    script_dir = @shared[:script_dir]

    if tool == :hybrid
      # Run both semantic and keyword searches and combine results
      semantic_query = search_plan[:semantic_query]
      keyword_query = search_plan[:keyword_query]

      LOG.info "Running semantic search with query: \"#{semantic_query}\""
      semantic_cmd = build_semantic_search_command(search_plan, script_dir, collection, top_k)
      LOG.debug "Semantic search command: #{semantic_cmd}"

      semantic_output = run_cmd(semantic_cmd)
      semantic_results = JSON.parse(semantic_output)
      LOG.debug "Raw semantic search results: #{semantic_results.length} conversations found"

      LOG.info "Running keyword search with query: \"#{keyword_query}\""
      keyword_cmd = "#{script_dir}/search-github-conversations #{Shellwords.escape(keyword_query)}"
      LOG.debug "Keyword search command: #{keyword_cmd}"

      keyword_output = run_cmd(keyword_cmd)
      keyword_results = JSON.parse(keyword_output)
      LOG.debug "Raw keyword search results: #{keyword_results.length} conversations found"

      # Convert keyword search results to match semantic search format
      keyword_normalized = keyword_results.map do |result|
        {
          "payload" => {
            "url" => result["url"],
            "summary" => "" # No summary available from keyword search - will be enriched later
          },
          "score" => 0.0 # No relevance score from keyword search
        }
      end

      # Combine results and deduplicate by URL
      combined_results = semantic_results + keyword_normalized
      url_to_result = {}
      combined_results.each do |result|
        url = result.dig("payload", "url")
        next unless url
        # Keep the semantic result if we have both (it has score and summary)
        if !url_to_result[url] || result["score"] > 0
          url_to_result[url] = result
        end
      end

      search_results = url_to_result.values.first(top_k)
      LOG.info "Combined #{semantic_results.length} semantic + #{keyword_results.length} keyword results into #{search_results.length} unique conversations"

      # Enrich keyword search results with summaries
      search_results.each do |result|
        url = result.dig("payload", "url")
        next unless url

        # Skip if already has a summary (from semantic search)
        next if result.dig("payload", "summary") && !result.dig("payload", "summary").empty?

        # Get or generate summary for this conversation
        summary = get_or_generate_summary(
          url,
          collection,
          script_dir,
          @shared[:executive_summary_prompt_path],
          @shared[:cache_path]
        )

        # Update the result with the enriched summary
        result["payload"]["summary"] = summary
      end

    elsif tool == :keyword
      query = search_plan[:query]
      LOG.info "Running keyword search with query..."
      search_cmd = "#{script_dir}/search-github-conversations #{Shellwords.escape(query)}"
      LOG.debug "Search command: #{search_cmd}"

      search_output = run_cmd(search_cmd)
      keyword_results = JSON.parse(search_output)

      LOG.debug "Raw keyword search results: #{keyword_results.length} conversations found"

      # Convert keyword search results to match semantic search format
      search_results = keyword_results.map do |result|
        {
          "payload" => {
            "url" => result["url"],
            "summary" => "" # No summary available from keyword search - will be enriched later
          },
          "score" => 0.0 # No relevance score from keyword search
        }
      end

      # Limit results to top_k
      search_results = search_results.first(top_k)

      # Enrich keyword search results with summaries
      search_results.each do |result|
        url = result.dig("payload", "url")
        next unless url

        # Get or generate summary for this conversation
        summary = get_or_generate_summary(
          url,
          collection,
          script_dir,
          @shared[:executive_summary_prompt_path],
          @shared[:cache_path]
        )

        # Update the result with the enriched summary
        result["payload"]["summary"] = summary
      end
    else
      query = search_plan[:query]
      LOG.info "Running semantic search with query..."

      # Extract qualifiers from the query for semantic search
      semantic_query_info = build_semantic_query(query)
      LOG.debug "Extracted semantic query: '#{semantic_query_info[:semantic_query]}'"
      LOG.debug "Extracted repo filter: #{semantic_query_info[:repo_filter]}" if semantic_query_info[:repo_filter]
      LOG.debug "Extracted author filter: #{semantic_query_info[:author_filter]}" if semantic_query_info[:author_filter]

      # Build updated search plan with extracted qualifiers
      updated_search_plan = search_plan.merge(semantic_query_info)

      search_cmd = build_semantic_search_command(updated_search_plan, script_dir, collection, top_k)
      LOG.debug "Search command: #{search_cmd}"

      search_output = run_cmd(search_cmd)
      search_results = JSON.parse(search_output)

      LOG.debug "Raw semantic search results: #{search_results.length} conversations found"
    end

    # Deduplicate URLs we already have
    existing_urls = @shared[:memory][:hits].map { |hit| hit[:url] }.to_set
    new_results = search_results.reject { |result| existing_urls.include?(result.dig("payload", "url")) }

    LOG.info "Found #{new_results.length} new conversations after deduplication"

    if new_results.empty?
      LOG.info "No new results found - all were duplicates"
      return []
    end

    LOG.debug do
      result_details = new_results.map.with_index do |result, i|
        "  #{i + 1}. URL: #{result.dig('payload', 'url')}\n" \
        "     Score: #{result['score']}\n" \
        "     Summary: #{result.dig('payload', 'summary')&.slice(0, 100)}..."
      end.join("\n\n")
      "New results found:\n#{result_details}"
    end

    # Fetch detailed data for new results
    new_enriched = []
    new_results.each_with_index do |result, i|
      url = result.dig("payload", "url")
      next unless url

      LOG.debug "Fetching details for new result #{i + 1}/#{new_results.length}: #{url}"

      begin
        fetch_cmd = "#{@shared[:script_dir]}/fetch-github-conversation"
        if @shared[:cache_path]
          fetch_cmd += " --cache-path #{Shellwords.escape(@shared[:cache_path])}"
        end
        fetch_cmd += " #{Shellwords.escape(url)}"

        conversation_json = run_cmd_safe(fetch_cmd)
        conversation_data = JSON.parse(conversation_json)

        metadata = extract_conversation_metadata(conversation_data)

        LOG.debug do
          "✓ Successfully fetched: #{metadata[:title]}\n" \
          "  Type: #{metadata[:type]}\n" \
          "  State: #{metadata[:state]}\n" \
          "  Comments: #{metadata[:comments_count]}"
        end

        new_enriched << {
          url: url,
          summary: result.dig("payload", "summary") || "",
          score: result["score"],
          conversation: conversation_data
        }
      rescue => e
        LOG.warn "Failed to fetch #{url}: #{e.message}"
      end
    end

    LOG.info "Successfully enriched #{new_enriched.length}/#{new_results.length} new conversations"

    # Enrich any conversations that still have empty summaries (from keyword searches)
    new_enriched.each do |enriched_result|
      url = enriched_result[:url]
      next unless url

      # Skip if already has a summary (from semantic search)
      next if enriched_result[:summary] && !enriched_result[:summary].empty?

      # Get or generate summary for this conversation
      summary = get_or_generate_summary(
        url,
        @shared[:collection],
        @shared[:script_dir],
        @shared[:executive_summary_prompt_path],
        @shared[:cache_path]
      )

      # Update the result with the enriched summary
      enriched_result[:summary] = summary
    end



    new_enriched
  end

  def post(shared, prep_res, exec_res)
    return "final" if prep_res.nil? # No search plan

    # Store the query used for memory tracking
    query = prep_res[:semantic_query] || prep_res[:query] || "Unknown query"

    # Add new findings to memory
    shared[:memory][:hits].concat(exec_res)
    shared[:memory][:search_queries] << query

    # Add research notes
    if exec_res.any?
      notes = exec_res.map { |hit| "#{hit[:url]}: #{hit[:summary]}" }.join("\n")
      shared[:memory][:notes] << "Research iteration: #{notes}"
      LOG.info "Added #{exec_res.length} new conversations to memory"
    else
      LOG.info "No new conversations added this iteration"
    end

    # Increment depth
    shared[:current_depth] = (shared[:current_depth] || 0) + 1

    LOG.debug do
      "Current memory state:\n" \
      "  Total conversations: #{shared[:memory][:hits].length}\n" \
      "  Total search queries: #{shared[:memory][:search_queries].length}\n" \
      "  Current depth: #{shared[:current_depth]}/#{shared[:max_depth]}"
    end

    # Continue if under max depth, otherwise go to final report
    if shared[:current_depth] < shared[:max_depth] && exec_res.any?
      LOG.info "Continuing to next research iteration..."
      "continue" # Go back to planning for next iteration
    else
      LOG.info "Research complete, moving to final report..."

      # Clear unsupported claims if we were researching them (to avoid re-triggering claim verification loop)
      if shared[:unsupported_claims] && shared[:unsupported_claims].any?
        LOG.info "Clearing unsupported claims after research attempt"
        shared[:unsupported_claims] = []
      end

      "final"
    end
  end
end

class ContextCompactionNode < Pocketflow::Node
  def prep(shared)
    @shared = shared
    LOG.info "=== CONTEXT COMPACTION PHASE ==="

    compaction_attempts = shared[:compaction_attempts] || 0
    max_compaction_attempts = 3

    if compaction_attempts >= max_compaction_attempts
      LOG.error "Maximum compaction attempts (#{max_compaction_attempts}) reached. Cannot reduce context further."
      return nil
    end

    hits = shared[:memory][:hits]

    if hits.length <= 3
      LOG.warn "Cannot compact further—only #{hits.length} conversations remain."
      LOG.warn "Proceeding with minimal context and hoping for the best..."
      return "proceed_anyway"
    end

    LOG.info "Attempt #{compaction_attempts + 1}/#{max_compaction_attempts}: Compacting research context to fit model limits"
    LOG.info "Starting with #{hits.length} conversations"

    # Sort conversations by priority using composite scoring strategy
    sort_conversations_by_priority!(hits)

    LOG.debug do
      priority_list = hits.first(5).map.with_index do |hit, i|
        score_info = hit[:score] ? "score: #{hit[:score]}" : "no score"
        summary_info = hit[:summary].to_s.strip.empty? ? "no summary" : "has summary"
        "  #{i + 1}. #{hit[:url]} (#{score_info}, #{summary_info})"
      end.join("\n")
      "Top 5 conversations by priority:\n#{priority_list}"
    end

    # Determine compaction strategy based on attempt number
    if compaction_attempts == 0
      # First attempt: Remove bottom 30% of conversations
      removal_count = (hits.length * 0.3).ceil
      strategy = "Remove bottom 30% by priority"
    elsif compaction_attempts == 1
      # Second attempt: Remove bottom 50% of conversations and strip conversation details
      removal_count = (hits.length * 0.5).ceil
      strategy = "Remove bottom 50% by priority and strip conversation details"
    else
      # Final attempt: Keep only top 25% and minimal data
      removal_count = hits.length - (hits.length * 0.25).ceil
      strategy = "Keep only top 25% with minimal data"
    end

    LOG.info "Strategy: #{strategy}"
    LOG.info "Will remove #{removal_count} conversations, keeping #{hits.length - removal_count}"

    # Remove lower-priority conversations
    removed_conversations = hits.pop(removal_count)

    LOG.debug do
      removed_list = removed_conversations.map.with_index do |hit, i|
        score_info = hit[:score] ? "score: #{hit[:score]}" : "no score"
        "  #{i + 1}. #{hit[:url]} (#{score_info})"
      end.join("\n")
      "Removed conversations:\n#{removed_list}"
    end

    # For second and third attempts, also strip conversation details
    if compaction_attempts >= 1
      LOG.info "Stripping conversation details to reduce context size..."

      hits.each do |hit|
        if hit[:conversation]
          # Keep only essential conversation metadata, remove full content
          conversation = hit[:conversation]
          essential_data = {}

          # Preserve main conversation object with minimal fields
          if conversation["issue"]
            essential_data["issue"] = {
              "title" => conversation["issue"]["title"],
              "state" => conversation["issue"]["state"],
              "url" => conversation["issue"]["url"],
              "created_at" => conversation["issue"]["created_at"],
              "updated_at" => conversation["issue"]["updated_at"]
            }
          elsif conversation["pr"]
            essential_data["pr"] = {
              "title" => conversation["pr"]["title"],
              "state" => conversation["pr"]["state"],
              "url" => conversation["pr"]["url"],
              "created_at" => conversation["pr"]["created_at"],
              "updated_at" => conversation["pr"]["updated_at"],
              "merged" => conversation["pr"]["merged"]
            }
          elsif conversation["discussion"]
            essential_data["discussion"] = {
              "title" => conversation["discussion"]["title"],
              "url" => conversation["discussion"]["url"],
              "created_at" => conversation["discussion"]["created_at"],
              "updated_at" => conversation["discussion"]["updated_at"]
            }
          end

          # Keep comment count but not full comment content
          essential_data["comments_count"] = conversation["comments"]&.length || 0
          essential_data["reviews_count"] = conversation["reviews"]&.length || 0
          essential_data["review_comments_count"] = conversation["review_comments"]&.length || 0

          hit[:conversation] = essential_data
        end
      end
    end

    # Update shared memory with compacted data
    shared[:memory][:hits] = hits

    {
      strategy: strategy,
      removed_count: removal_count,
      remaining_count: hits.length,
      compaction_attempt: compaction_attempts + 1
    }
  end

  def exec(compaction_info)
    return nil if compaction_info.nil?
    return "proceed_anyway" if compaction_info == "proceed_anyway"

    LOG.info "Applied compaction strategy: #{compaction_info[:strategy]}"
    LOG.info "Removed #{compaction_info[:removed_count]} conversations"
    LOG.info "#{compaction_info[:remaining_count]} conversations remaining"

    compaction_info
  end

  def post(shared, prep_res, exec_res)
    return "proceed_anyway" if prep_res.nil? || exec_res == "proceed_anyway"

    compaction_attempts = (shared[:compaction_attempts] || 0) + 1
    shared[:compaction_attempts] = compaction_attempts

    LOG.info "✓ Context compaction attempt #{compaction_attempts} completed"

    LOG.debug do
      "Context compaction summary:\n" \
      "  - Strategy applied: #{exec_res[:strategy]}\n" \
      "  - Conversations removed: #{exec_res[:removed_count]}\n" \
      "  - Conversations remaining: #{exec_res[:remaining_count]}\n" \
      "  - Compaction attempts so far: #{compaction_attempts}/3"
    end

    # Sleep briefly after compaction to avoid immediately hitting rate limits again
    sleep_duration = 60
    LOG.info "Waiting #{sleep_duration} seconds after compaction before retrying..."
    sleep(sleep_duration)

    "retry" # Signal to retry the final report
  end
end

class ClaimVerifierNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    LOG.info "=== CLAIM VERIFICATION PHASE ==="

    # Check if we already have a draft answer to verify
    unless shared[:draft_answer]
      LOG.error "No draft answer found for claim verification"
      return nil
    end

    LOG.info "Extracting factual claims from draft answer for verification..."

    # Extract claims from the draft answer
    claims = extract_claims_from_report(shared[:draft_answer], shared[:models][:fast])

    if claims.empty?
      LOG.info "No factual claims found in draft answer, proceeding to final report"
      return :no_claims
    end

    LOG.info "Found #{claims.length} claims to verify"
    LOG.debug do
      claims_list = claims.map.with_index { |claim, i| "  #{i + 1}. #{claim}" }.join("\n")
      "Claims to verify:\n#{claims_list}"
    end

    claims
  end

  def exec(claims)
    return :no_claims if claims == :no_claims || claims.nil?

    LOG.info "Verifying #{claims.length} claims against available evidence..."

    collection = @shared[:collection]
    script_dir = @shared[:script_dir]
    fast_model = @shared[:models][:fast]

    supported_claims = []
    unsupported_claims = []

    claims.each_with_index do |claim, i|
      LOG.debug "Verifying claim #{i + 1}/#{claims.length}: #{claim.slice(0, 100)}..."

      # Search for evidence related to this claim
      evidence = search_evidence_for_claim(claim, collection, script_dir, 3)

      # Verify the claim against the evidence
      is_supported = verify_claim_against_evidence(claim, evidence, fast_model)

      if is_supported
        supported_claims << claim
        LOG.debug "✓ Claim #{i + 1} SUPPORTED"
      else
        unsupported_claims << claim
        LOG.debug "✗ Claim #{i + 1} UNSUPPORTED"
      end
    end

    LOG.info "Verification complete: #{supported_claims.length} supported, #{unsupported_claims.length} unsupported"

    if unsupported_claims.any?
      LOG.warn "Found unsupported claims:"
      unsupported_claims.each_with_index do |claim, i|
        LOG.warn "  #{i + 1}. #{claim}"
      end
    end

    {
      total_claims: claims.length,
      supported_claims: supported_claims,
      unsupported_claims: unsupported_claims
    }
  end

  def post(shared, prep_res, exec_res)
    return "ok" if prep_res.nil? || exec_res == :no_claims

    verification_attempts = shared[:verification_attempts] || 0

    # Store verification results
    shared[:claim_verification] = exec_res
    shared[:unsupported_claims] = exec_res[:unsupported_claims]

    if exec_res[:unsupported_claims].empty?
      LOG.info "✓ All claims verified successfully, proceeding to final report"
      return "ok"
    else
      LOG.warn "Found #{exec_res[:unsupported_claims].length} unsupported claims"

      # Allow only one retry to gather better evidence
      if verification_attempts < 1
        shared[:verification_attempts] = verification_attempts + 1
        LOG.info "Routing back to planner to gather better evidence for unsupported claims (attempt #{verification_attempts + 1}/1)"
        return "fix"
      else
        LOG.warn "Maximum verification attempts reached, proceeding with unsupported claims noted"
        return "ok"
      end
    end
  end
end

class FinalReportNode < Pocketflow::Node
  def prep(shared)
    @shared = shared # Store shared context
    LOG.info "=== FINAL REPORT PHASE ==="
    LOG.info "Generating final report from all gathered data..."

    compaction_note = ""
    if shared[:compaction_attempts] && shared[:compaction_attempts] > 0
      compaction_note = " (after #{shared[:compaction_attempts]} context compaction attempts)"
    end

    LOG.info "Research summary: #{shared[:memory][:hits].length} conversations analyzed#{compaction_note}, #{shared[:memory][:search_queries].length} queries used, #{shared[:current_depth] || 0} deep research iterations"

    LOG.debug do
      sources_list = shared[:memory][:hits].map.with_index do |hit, i|
        "  #{i + 1}. #{hit[:url]} (score: #{hit[:score]})"
      end.join("\n")
      "All conversation sources:\n#{sources_list}"
    end

    # Compile all findings
    all_findings = shared[:memory][:hits].map do |hit|
      <<~FINDING
      **Source**: #{hit[:url]}
      **Summary**: #{hit[:summary]}
      **Relevance Score**: #{hit[:score]}

      **Conversation Details**:
      #{JSON.pretty_generate(hit[:conversation])}
      FINDING
    end.join("\n\n---\n\n")

    prompt = fill_template(FINAL_REPORT_PROMPT, {
      request: shared[:request],
      clarifications: shared[:clarifications] || "None provided",
      all_findings: all_findings
    })

    LOG.debug "Calling LLM to generate final report..."
    prompt
  end

  def exec(prompt)
    begin
      # Use reasoning model for final report - this requires complex analysis and synthesis of all gathered data
      draft_answer = call_llm(prompt, @shared[:models][:reasoning])

      # Store the draft answer for claim verification
      @shared[:draft_answer] = draft_answer

      draft_answer
    rescue => e
      if context_too_large_error?(e.message) || rate_limit_error?(e.message)
        if rate_limit_error?(e.message)
          LOG.warn "Rate limit encountered: #{e.message}"
          LOG.info "Will attempt to compact context to reduce token usage and retry..."
        else
          LOG.warn "Context too large for model: #{e.message}"
          LOG.info "Will attempt to compact context and retry..."
        end

        # Store the error for reference
        @shared[:last_context_error] = e.message

        # Return a special signal to indicate compaction is needed
        return :context_too_large
      else
        # Re-raise other errors
        LOG.error "Unexpected error during final report generation: #{e.message}"
        raise e
      end
    end
  end

  def post(shared, prep_res, exec_res)
    # Check if we need to compact context
    if exec_res == :context_too_large
      LOG.info "Context too large, routing to compaction..."
      return "compact"
    end

    # Check if this is the first time generating the report (route to claim verification)
    unless shared[:claim_verification_completed]
      LOG.info "Routing to claim verification before final output"
      shared[:claim_verification_completed] = true
      return "verify"
    end

    # Final output with claim verification results
    LOG.info "=== FINAL REPORT ===\n\n"
    puts exec_res

    # Add note about unsupported claims if any
    if shared[:unsupported_claims] && shared[:unsupported_claims].any?
      puts "\n\n---\n\n"
      puts "**Note**: The following #{shared[:unsupported_claims].length} claims could not be fully verified against the available evidence:"
      shared[:unsupported_claims].each_with_index do |claim, i|
        puts "#{i + 1}. #{claim}"
      end
    end

    compaction_note = ""
    if shared[:compaction_attempts] && shared[:compaction_attempts] > 0
      compaction_note = " (after #{shared[:compaction_attempts]} context compaction attempts)"
    end

    verification_note = ""
    if shared[:claim_verification]
      verification_note = ", #{shared[:claim_verification][:total_claims]} claims verified (#{shared[:claim_verification][:supported_claims].length} supported, #{shared[:claim_verification][:unsupported_claims].length} unsupported)"
    end

    LOG.info "\n\n✓ Research complete! Total conversations analyzed: #{shared[:memory][:hits].length}#{compaction_note}#{verification_note}"

    "complete"
  end
end

# === Main Script ===

# Parse command-line options
options = {
  collection: nil,
  limit: 5,
  max_depth: 2,
  editor_file: nil,
  clarifying_qa: nil,
  verbose: false,
  fast_model: ENV["FAST_LLM_MODEL"],
  reasoning_model: ENV["LLM_MODEL"],
  search_mode: "hybrid",
  cache_path: nil,
  executive_summary_prompt_path: nil
}

opt_parser = OptionParser.new do |opts|
  opts.banner = "Usage: #{File.basename($0)} \"REQUEST\" --collection COLLECTION [options]"

  opts.on("--collection NAME", "Qdrant collection name (required)") do |v|
    options[:collection] = v
  end

  opts.on("-n", "--limit N", Integer, "Max results per search (default: 10)") do |v|
    options[:limit] = v
  end

  opts.on("--max-depth N", Integer, "Max deep-research passes (default: 10)") do |v|
    options[:max_depth] = v
  end

  opts.on("--editor-file PATH", "Use fixed file instead of Tempfile") do |v|
    options[:editor_file] = v
  end

  opts.on("--clarifying-qa PATH", "Path to file with clarifying Q&A to bypass interactive step") do |v|
    options[:clarifying_qa] = v
  end

  opts.on("--verbose", "Show debug logs") do
    options[:verbose] = true
  end

  opts.on("--fast-model MODEL", "Fast LLM model for light reasoning (default: ENV['FAST_LLM_MODEL'] or llm default)") do |v|
    options[:fast_model] = v
  end

  opts.on("--reasoning-model MODEL", "Reasoning LLM model for complex analysis (default: ENV['LLM_MODEL'] or llm default)") do |v|
    options[:reasoning_model] = v
  end

  opts.on("--search-mode MODE", ["semantic", "keyword", "hybrid"], "Override search mode (semantic, keyword, or hybrid - default: hybrid)") do |v|
    options[:search_mode] = v
  end

  opts.on("--cache-path PATH", "Root path for caching fetched data") do |v|
    options[:cache_path] = v
  end

  opts.on("--executive-summary-prompt-path PATH", "Path to LLM prompt file for executive summary generation") do |v|
    options[:executive_summary_prompt_path] = v
  end

  opts.on("-h", "--help", "Show this help message") do
    puts opts
    exit
  end
end

begin
  opt_parser.parse!
rescue OptionParser::InvalidOption => e
  abort "#{e.message}\n\n#{opt_parser}"
end

# Set logger level based on verbose flag
LOG.level = options[:verbose] ? Logger::DEBUG : Logger::INFO

# Validate required arguments
if ARGV.empty?
  abort opt_parser.to_s
end

request = ARGV.join(" ")

if request.strip.empty?
  abort "Error: Empty request provided"
end

unless options[:collection]
  abort "Error: --collection is required\n\n#{opt_parser}"
end

# Set up shared context
script_dir = File.expand_path(File.dirname(__FILE__))

shared = {
  request: request,
  collection: options[:collection],
  top_k: options[:limit],
  max_depth: options[:max_depth],
  editor_file: options[:editor_file],
  clarifying_qa: options[:clarifying_qa],
  verbose: options[:verbose],
  search_mode: options[:search_mode],
  cache_path: options[:cache_path],
  executive_summary_prompt_path: options[:executive_summary_prompt_path],
  models: {
    fast: options[:fast_model],
    reasoning: options[:reasoning_model]
  },
  script_dir: script_dir
}

# Build the workflow
initial_node = InitialResearchNode.new
clarify_node = AskClarifyingNode.new
planner_node = PlannerNode.new
retriever_node = RetrieverNode.new
compaction_node = ContextCompactionNode.new
claim_verifier_node = ClaimVerifierNode.new
final_node = FinalReportNode.new
end_node = GitHubDeepResearchAgent::EndNode.new

# Link the nodes
initial_node.next(clarify_node)
clarify_node.next(planner_node)
planner_node.next(retriever_node)
retriever_node.on("continue", planner_node) # Loop back to planner for next iteration
retriever_node.on("final", final_node)

# Add claim verification flow
final_node.on("verify", claim_verifier_node)  # Route to claim verification after draft report
final_node.on("complete", end_node)           # Route to clean termination
claim_verifier_node.on("ok", final_node)     # Continue to final output after verification
claim_verifier_node.on("fix", planner_node)  # Route back to planner to gather evidence for unsupported claims

# Add compaction handling
final_node.on("compact", compaction_node)         # Route to compaction when context too large
compaction_node.on("retry", final_node)          # Retry final report after compaction
compaction_node.on("proceed_anyway", final_node) # Proceed with minimal context if compaction fails

# Set end node as the final termination point
final_node.next(end_node)

# Create and run the flow
flow = Pocketflow::Flow.new(initial_node)

begin
  LOG.info "=== GITHUB CONVERSATIONS RESEARCH AGENT ==="
  LOG.info "Request: #{request}"
  LOG.info "Collection: #{options[:collection]}"
  LOG.info "Max results per search: #{options[:limit]}"
  LOG.info "Max deep research iterations: #{options[:max_depth]}"
  LOG.info "Fast model: #{shared[:models][:fast] || 'default'}"
  LOG.info "Reasoning model: #{shared[:models][:reasoning] || 'default'}"

  flow.run(shared)
rescue Interrupt
  LOG.error "\nResearch interrupted by user"
  exit 1
rescue => e
  LOG.error "Error: #{e.message}"
  LOG.debug e.backtrace.join("\n") if shared[:verbose]
  exit 1
end
