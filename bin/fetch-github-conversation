#!/usr/bin/env ruby

# fetch-github-conversation: Fetch and export GitHub issue, pull request, or discussion data as structured JSON
# and optionally cache the results for future use.
#
# This script retrieves detailed data from a GitHub issue, pull request, or discussion URL using the GitHub CLI (`gh`).
# It outputs a single JSON object containing the main conversation and all comments, suitable for archiving or further processing.
#
# Features:
# - Accepts a GitHub URL for an issue, pull request, or discussion.
# - Parses the URL to determine the type (issue, pull, discussion) and relevant identifiers.
# - Fetches all relevant data using the GitHub CLI:
#   - For issues: metadata and all comments.
#   - For pull requests: metadata, comments, reviews, review comments, and the diff.
#   - For discussions: metadata and all comments (via GraphQL).
# - Outputs a single, pretty-printed JSON object to stdout.
#
# Examples
#
#   fetch-github-conversation <github_conversation_url>
#   fetch-github-conversation https://github.com/octocat/Hello-World/issues/42 > issue-42.json
#
#   # Using flags:
#   fetch-github-conversation --cache-path ./cache --updated-at 2025-05-26T00:00:00Z https://github.com/octocat/Hello-World/issues/42
#   fetch-github-conversation --cache-path ./cache octocat/Hello-World/issues/42
#
# Arguments
#
#   github_conversation_url - The URL of the GitHub issue, pull request, or discussion to fetch.
#
# Requirements
#
# - Ruby
# - The `gh` command-line tool (GitHub CLI) with appropriate authentication
#
# The script will abort with an error message if the URL is not recognized or if any command fails.

# Require standard libraries for JSON parsing, shell execution, URI handling, file utilities, and option parsing.
require "json"
require "open3"
require "uri"
require "fileutils"
require "optparse"

# Public: Runs a shell command and returns stdout. Aborts if the command fails.
#
# cmd - The shell command to run (String).
#
# Returns the standard output of the command (String).
# Raises SystemExit if the command fails.
def run_cmd(cmd)
  stdout, stderr, status = Open3.capture3(cmd)
  unless status.success?
    abort "Command failed: #{cmd}\n#{stderr}"
  end
  stdout
end

# Public: Parses a GitHub conversation URL and extracts owner, repo, type, and number.
#
# url - The GitHub issue, pull, or discussion URL (String).
#
# Returns an Array of [owner, repo, type, number].
# Raises SystemExit if the URL is not recognized.
def parse_github_url(url)
  uri = URI.parse(url)
  path = uri.path
  m = path.match(%r{^/([^/]+)/([^/]+)/(issues|pull|discussions)/([0-9]+)})
  unless m
    abort "Unrecognized GitHub conversation URL: #{url}"
  end
  owner, repo, type, number = m.captures
  [owner, repo, type, number]
end

# Internal: Recursively convert all hash keys from camelCase to snake_case.
#
# obj - The object to convert (Array, Hash, or other).
#
# Returns the object with all hash keys in snake_case.
def deep_snake_case(obj)
  case obj
  when Array
    obj.map { |v| deep_snake_case(v) }
  when Hash
    obj.each_with_object({}) do |(k, v), h|
      new_key = k.gsub(/([A-Z])/, '_\1').downcase
      h[new_key] = deep_snake_case(v)
    end
  else
    obj
  end
end

# Internal: Recursively flatten GraphQL node structures by pulling node contents up one level.
#
# obj - The object to flatten (Array, Hash, or other).
#
# Returns the object with GraphQL node wrappers removed.
def flatten_graphql_nodes(obj)
  case obj
  when Array
    obj.map { |v| flatten_graphql_nodes(v) }
  when Hash
    result = {}
    obj.each do |k, v|
      if v.is_a?(Hash) && v.key?("nodes") && v["nodes"].is_a?(Array)
        # For structures like comments: { nodes: [...] } or replies: { nodes: [...] }
        # Replace with just the array content
        result[k] = flatten_graphql_nodes(v["nodes"])
      elsif k == "nodes" && v.is_a?(Array)
        # This should not happen with the above condition, but handle it just in case
        return flatten_graphql_nodes(v)
      else
        result[k] = flatten_graphql_nodes(v)
      end
    end
    result
  else
    obj
  end
end

# Public: Fetches a GitHub issue and all its comments.
#
# owner  - The repository owner (String).
# repo   - The repository name (String).
# number - The issue number (String).
#
# Returns a Hash with "issue" and "comments" keys.
def fetch_issue(owner, repo, number)
  issue = JSON.parse(run_cmd("gh api repos/#{owner}/#{repo}/issues/#{number}"))
  comments = JSON.parse(run_cmd("gh api repos/#{owner}/#{repo}/issues/#{number}/comments --paginate"))
  { "issue" => issue, "comments" => comments }
end

# Public: Fetches a GitHub discussion and all its comments using GraphQL.
#
# owner  - The repository owner (String).
# repo   - The repository name (String).
# number - The discussion number (String).
#
# Returns a Hash with "discussion" and "comments" keys.
def fetch_discussion(owner, repo, number)
  json = run_cmd("gh api graphql -f query='query { repository(owner: \"#{owner}\", name: \"#{repo}\") { discussion(number: #{number}) { id number title body author { login } createdAt url updatedAt comments(first: 100) { nodes { id author { login } body replyTo { id } createdAt url updatedAt replies(first: 100) { nodes { id author { login } body replyTo { id } createdAt url updatedAt } } } } } } } }'")
  data = JSON.parse(json)
  discussion = data.dig("data", "repository", "discussion")

  # Flatten the GraphQL nodes structure and extract comments
  discussion_flattened = flatten_graphql_nodes(discussion)
  comments = discussion_flattened["comments"] || []

  discussion_out = discussion_flattened.dup
  discussion_out.delete("comments")
  # Convert all keys to snake_case for consistency
  {
    discussion: deep_snake_case(discussion_out),
    comments: deep_snake_case(comments)
  }
end

# Public: Fetches a GitHub pull request, all comments, reviews, review comments, and the diff.
#
# owner  - The repository owner (String).
# repo   - The repository name (String).
# number - The pull request number (String).
#
# Returns a Hash with "pr", "comments", "reviews", "review_comments", "diff", and "commits" keys.
def fetch_pr(owner, repo, number)
  pr = JSON.parse(run_cmd("gh api repos/#{owner}/#{repo}/pulls/#{number}"))
  comments = JSON.parse(run_cmd("gh api repos/#{owner}/#{repo}/issues/#{number}/comments --paginate"))
  reviews = JSON.parse(run_cmd("gh api repos/#{owner}/#{repo}/pulls/#{number}/reviews --paginate"))
  review_comments = JSON.parse(run_cmd("gh api repos/#{owner}/#{repo}/pulls/#{number}/comments --paginate"))
  diff = run_cmd("gh pr diff #{number} -R #{owner}/#{repo}")

  # Fetch detailed commit information
  commits = JSON.parse(run_cmd("gh api repos/#{owner}/#{repo}/pulls/#{number}/commits --paginate"))

  { "pr" => pr, "comments" => comments, "reviews" => reviews, "review_comments" => review_comments, "diff" => diff, "commits" => commits }
end

# Internal: Load cached data if present.
#
# path - The path to the cache file (String).
#
# Returns the parsed JSON data or nil if not present.
def load_cache(path)
  return nil unless path && File.exist?(path)
  JSON.parse(File.read(path))
end

# Internal: Save data to cache.
#
# path - The path to the cache file (String).
# data - The data to save (Object).
def save_cache(path, data)
  FileUtils.mkdir_p(File.dirname(path))
  File.write(path, JSON.pretty_generate(data))
end

# Internal: Accept either a URL or <owner>/<repo>/<type>/<number> as input.
#
# input - The input string (String).
#
# Returns an Array of [owner, repo, type, number].
def parse_input(input)
  if input =~ %r{^https://github.com/}
    parse_github_url(input)
  elsif input =~ %r{^([^/]+)/([^/]+)/(issues|pull|discussions)/([0-9]+)$}
    m = input.match(%r{^([^/]+)/([^/]+)/(issues|pull|discussions)/([0-9]+)$})
    [m[1], m[2], m[3], m[4]]
  else
    abort "Input must be a GitHub URL or <owner>/<repo>/<type>/<number>"
  end
end

# Internal: Returns the updated_at timestamp from the fetched data (as ISO8601 string).
#
# data - The fetched data (Hash).
# type - The conversation type (String).
#
# Returns the updated_at timestamp (String) or nil.
def get_updated_at(data, type)
  case type
  when "issues"
    data["issue"]["updated_at"]
  when "pull"
    data["pr"]["updated_at"]
  when "discussions"
    data["discussion"]["updated_at"]
  else
    nil
  end
end

# Internal: Extract a filtered version of a GitHub conversation (issue, PR, or discussion).
#
# data - The full data Hash returned by `fetch_*`.
# type - One of "pull", "issues", or "discussions".
#
# Returns a compact Hash with only key metadata preserved.
def extract_filtered_conversation(data, type)
  result = {}

  conversation_key = {
    "pull" => "pr",
    "issues" => "issue",
    "discussions" => "discussion",
  }[type]

  conversation = data[conversation_key]
  return {} unless conversation

  # Base fields common to all types
  result[conversation_key] = {
    "title" => conversation["title"],
    "body" => conversation["body"],
    "html_url" => conversation["html_url"] || conversation["url"],
    "created_at" => conversation["created_at"],
    "updated_at" => conversation["updated_at"],
    "closed_at" => conversation["closed_at"],
    "state" => conversation["state"],
    "author" => conversation.dig("user", "login") || conversation.dig("author", "login")
  }

  # Assignees and labels (shared by issues & PRs)
  if conversation["assignees"]
    result[conversation_key]["assignees"] = conversation["assignees"].map { |a| a["login"] }
  end

  if conversation["labels"]
    result[conversation_key]["labels"] = conversation["labels"].map { |l| l["name"] }
  end

  # PR-only fields
  if type == "pull"
    result[conversation_key]["merged_at"] = conversation["merged_at"]
    result[conversation_key]["draft"] = conversation["draft"]
    result[conversation_key]["merged_by"] = conversation.dig("merged_by", "login")
    result[conversation_key]["requested_reviewers"] = conversation.fetch("requested_reviewers", []).map { |r| r["login"] }
    result[conversation_key]["base_branch"] = conversation.dig("base", "ref")
    result[conversation_key]["head_branch"] = conversation.dig("head", "ref")
    result[conversation_key]["additions"] = conversation["additions"]
    result[conversation_key]["deletions"] = conversation["deletions"]
    result[conversation_key]["changed_files"] = conversation["changed_files"]
    result[conversation_key]["commits"] = conversation["commits"]

    # Include the full diff
    if data["diff"]
      result["diff"] = data["diff"]
    end

    # Include detailed commit metadata
    if data["commits"]
      result["commits"] = data["commits"].map do |commit|
        {
          "sha" => commit["sha"],
          "message" => commit.dig("commit", "message"),
          "author" => commit.dig("commit", "author", "name"),
          "author_email" => commit.dig("commit", "author", "email"),
          "authored_date" => commit.dig("commit", "author", "date"),
          "committer" => commit.dig("commit", "committer", "name"),
          "committer_email" => commit.dig("commit", "committer", "email"),
          "committed_date" => commit.dig("commit", "committer", "date"),
          "url" => commit["html_url"]
        }.compact
      end
    end
  end

  # Comments (shared)
  if data["comments"]
    result["comments"] = data["comments"].map do |c|
      comment = {
        "author" => c.dig("user", "login") || c.dig("author", "login"),
        "body" => c["body"],
        "created_at" => c["created_at"],
        "reply_to" => c.dig("reply_to", "id")
      }.compact

      # Include replies for discussion comments
      if c["replies"] && c["replies"].is_a?(Array)
        comment["replies"] = c["replies"].map do |r|
          {
            "author" => r.dig("user", "login") || r.dig("author", "login"),
            "body" => r["body"],
            "created_at" => r["created_at"],
            "reply_to" => r.dig("reply_to", "id")
          }.compact
        end
      end

      comment
    end
  end

  # PR reviews
  if type == "pull" && data["reviews"]
    result["reviews"] = data["reviews"].map do |r|
      {
        "author" => r.dig("user", "login"),
        "state" => r["state"],
        "body" => r["body"],
        "submitted_at" => r["submitted_at"]
      }.compact
    end
  end

  # PR review comments
  if type == "pull" && data["review_comments"]
    result["review_comments"] = data["review_comments"].map do |rc|
      {
        "author" => rc.dig("user", "login"),
        "path" => rc["path"],
        "position" => rc["position"],
        "line" => rc["line"],
        "body" => rc["body"],
        "created_at" => rc["created_at"]
      }.compact
    end
  end

  result
end

# CLI options hash for OptionParser.
options = {
  cache_path: nil,
  updated_at: nil
}

# Set up OptionParser for CLI arguments.
opt_parser = OptionParser.new do |opts|
  opts.banner = "Usage: #{$0} <github_conversation_url|owner/repo/type/number> [options]"
  opts.on("--cache-path PATH", "Root path for caching fetched data") { |v| options[:cache_path] = v }
  opts.on("--updated-at TIME", "Timestamp to compare against (ISO8601)") { |v| options[:updated_at] = v }
end
opt_parser.parse!
opt_parser.order!(ARGV)

# Find the first non-option argument as the input (positional argument).
input = ARGV[0]
if input.nil?
  abort opt_parser.to_s
end

# Remove the positional argument from ARGV so it doesn't interfere with later logic.
ARGV.delete(input)

# Parse the input into owner, repo, type, and number.
owner, repo, type, number = parse_input(input)

# Determine cache file path if caching is enabled.
cache_file = nil
if options[:cache_path]
  cache_file = File.join(options[:cache_path], "conversations", owner, repo, type, "#{number}.json")
end

# Load cached data if available.
should_fetch = true
cached_data = load_cache(cache_file)

if cached_data && options[:updated_at]
  # Compare updated_at from cache and flag.
  cached_time = get_updated_at(cached_data, type)
  should_fetch = options[:updated_at] > cached_time if cached_time
elsif cached_data
  should_fetch = false
end

# Fetch data from GitHub if needed, otherwise use cache.
if should_fetch
  data = case type
  when "issues"
    fetch_issue(owner, repo, number)
  when "discussions"
    fetch_discussion(owner, repo, number)
  when "pull"
    fetch_pr(owner, repo, number)
  else
    abort "Unknown conversation type: #{type}"
  end
  data = extract_filtered_conversation(data, type)
  save_cache(cache_file, data) if cache_file
else
  data = cached_data
end

# Fetch the conversation data based on the type and filter down
# to high-value metadata for use in other AI based workflows.
puts JSON.pretty_generate(data)
